<!DOCTYPE html>
<html lang="en">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta charset="utf-8">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    <!-- Custom CSS tyle Sheet link -->
    <link href="../style.css" type="text/css" rel="stylesheet">

    <!-- Title -->
    <title>Vivian's Blog</title>
  </head>

  <body>
    <div id="mySidebar" class="sidebar">
      <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">&times;</a>
      <a href="../index.html">Home</a></li>
      <a href="./gamified.html">Gamified Task Manager</a></li>
      <a href="./fidelity.html">Fidelity</a></li>
      <a href="./greencrew.html">Sustainable Claremont</a></li>
      <a href="../about.html">Info</a>
    </div>

    <div id="main">
      <button class="openbtn" onclick="openNav()">&#9776;</button>
      
      <!-- Project Header -->
      <div class="proj-header">
        <div class="proj-title">
            <h1>Fidelity Internship</h1>
        </div>
        <div class="proj-dates">
          <p>June 2020-Present</p>
      </div>

        <div class="proj-status">
            <p><em>STATUS: In Progress</em></p>
        </div>
      </div>

        <!-- shortcuts to top and bottom -->
        <div class="proj-shortcuts">
            <div class="proj-shortcuts-top">
                <p><a href="#July-17-2020-FI">&uArr;<br>To newest</a></p>
            </div>
            <div class="proj-shortcuts-bot">
                <p><a href="#June-08-2020-FI">To oldest<br>&dArr;</a></p>
            </div>
        </div>

        <!-- Blog posts -->
        <div class="posts">
          <div id= "July-17-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 17, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Friday</p>
                </div>
                <div class="post-title">
                    <p>Presentations and Printing</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  For most of today I continued working on my Spend classification tool trying to find the most efficient way to print the outputs. Working off of yesterday’s approach, today I worked on a method that prints the predictions into the original data’s existing spreadsheet for all 3 commodity codes using one file. This fulfills the request to have all of the predictions in adjacent rows to the original data for comparison. The method of training the model and making the predictions all in one file takes about 6 minutes to run which is much more efficient than yesterday’s approach. This also does not require clearing all the variables between each run. I also spent some time moving my trained models to joblib files for better efficiency. The method of training the model, saving this model, and running predictions in a different file takes an accumulated time of 5 minutes. One other benefit of this saved model approach is that it requires less memory. I think for the purposes of this project, using a saved model makes more sense. Not only is it more time and memory efficient on my sample data, but if you wanted to train the model on one data set and make predictions for a new data set, the saved model approach would be more efficient. 
                </p>
                <p>
                  Kiran granted me access to the database this afternoon and Rajiv sent me a license code. However, I’m a bit lost on how to get it set up. Rajiv recommended that I connect with Poornima about setting up the necessary tools, since she set it up pretty recently. I set up a meeting with her for Monday afternoon. Once I get this set up, I will be able to explore the database table that I have access to. I think it would be beneficial to access the database and look at the data before I work with it directly.
                </p>
                <p>
                  Today, I also got to showcase my project to Kiran and some of his India development team. I got some great suggestions about how to implement manual rules efficiently using dictionaries and stopwords, which I hadn’t even thought of! This gives me a good direction to go in with regards to implementing manual rules and I feel a lot less stuck. Overall, the presentation went pretty smoothly but I feel like I rushed some parts. Rajiv mentioned that I might be presenting my project to Janice and the rest of APT at the next bi-monthly meeting (which is during the last week of my internship). I think I’ll need to have a full powerpoint presentation prepared to stay on track for this meeting instead of just using talking points and demonstrations like I’ve been using so far. I’m a bit nervous for this presentation since it's a larger group of associates that I don’t know very well, but maybe I can practice with Nancy and/or Derik as the date approaches so I can fine-tune and practice my presentation. Something this demonstration made clear to me is that I’ll need to assume no prior knowledge of my project. I’m usually talking to Rajiv or Kiran about my project, but since they are already very familiar with my project, I’ll need to change the way present for other groups. Speaking of presentations, I worked on my end-of-internship presentation script a bit more today. I might practice this presentation with Nancy this week if she has time, otherwise I’ll run through it on my own. I have most of the presentation done, but I need to think about my demo and my results slides a bit more. Maybe I can even add a ‘Next Steps’ slide since I’ll be working on this project for another month. I plan to ask Rajiv what he thinks and confirm the length of the presentation during our 1:1 on Tuesday.
                </p>
                <p>
                  Derek mentioned the AI club during our meeting, so I spent some of my time today browsing their site and resources. Some of the resources seem very advanced and mention topics beyond my scope of knowledge. However, I found some interesting introductory articles about FCAT and Machine Learning that I plan to read Monday morning in my downtime. 
                </p>
                <p>
                  Next week there are a couple of things to look forward to. The first is the end-of-internship presentations. I’ll get the opportunity to present my project to CTG senior leadership and my fellow CTG interns. I’m excited to hear about what other CTG interns have been working on this summer! I’m also eager to start working with the database now that I have access. I’ve never worked directly with a database, so this will be a new experience for me. I’m happy with the progress I’ve made this week in regards to the transition to predicting commodity codes and different networking opportunities! :)
                </p>
            </div>
          </div>
        
          <div id= "July-16-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 16, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Thursday</p>
                </div>
                <div class="post-title">
                    <p>Formatting Predictions</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  For most of today, I worked on reducing runtime and attempted formatting my predictions in the way that Kiran requested. I also spent a decent amount of time working on my end-of-internship presentation In my previous version, the model was making the predictions for all 3 levels of commodity codes in one file, but this led to a very long runtime. Today I split the predictions into 3 files. I wrote the code in a similar format to my purchase code prediction model. The problem with this is that the predictions for each level of commodity codes are in separate files and it is inserted to the format of the original data. However, this process is a lot faster than writing all 3 levels at once. I also found that clearing the memory of previous variables using ‘%reset -f’ in the terminal between runs will make the process faster, so I may need to add a note for users to do this in between runs. I was able to finish most of my presentation today. There are a few portions that will be completed closer to the presentation date since they will talk about portions of the project that I have not yet completed. I might edit my presentation script tomorrow, but for now I am happy with where this presentation is.
                </p>
                <p>
                  Today, there was also a webinar for elevator pitches. They talked about the purpose of elevator pitches and the key components that should be present in an elevator pitch. Most of this information was pretty repetitive and included topics I had learned in interview preparation workshops but I thought the examples that they showed were informative. Rajiv got approval to extend my internship program by 2 weeks, so I will be working until August 21 instead of August 9. I’m excited to see what I can accomplish with this extra time. It seems like some other interns are also getting their internships extended like Renbin from the CTG intern group and Benjamin who I spoke to yesterday. I’m hoping that more interns will get their programs extended so we have extra time to connect!
                </p>
                <p>
                  Tomorrow, I plan to continue working on the formatting of my outputs so they can be printed into the same Excel sheet but the code can be run separately to keep efficiency. I still have yet to receive a response from Kiran, so I might reach out to him again tomorrow. I also have my demonstration for some members of the India development team tomorrow, so I will likely spend my morning preparing for this.
                </p>
            </div>
          </div>
        
          <div id= "July-15-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 15, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Wednesday</p>
                </div>
                <div class="post-title">
                    <p>Fixed Memory Error :)</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  This morning after the weekly update call, I got in contact with Techworks to try again to fix the memory allocation error. Fortunately, I was able to clearly communicate the error that I was having and how Rajiv suggested to resolve the error. Techworks was able to enter administrator access so that I could update my machine settings and resolve the memory allocation errors. With this error fixed, I should be able to move forward with my Spend classification tool to train and predict using more data.
                </p>
                <p>
                  This morning I also had my meeting with Derik and Danny. Danny works in Data Analytics and Insights and explained the work he is doing. He showed me how he runs automation software on information from databases. Maybe I can ask him for advice in terms of pulling data from a database, processing the data, and writing an output back to the database. He gave some great insight about how Fidelity uses automation in combination with machine learning and how Fidelity keeps an eye on new technology. Derek also brought up how data is a valuable commodity in today’s world since a lot of processes depend on the data we have stored. This gave me something new to consider. So far, I’ve been following a track to prepare for Software Engineering roles, but Data Engineering is a field that I might look more into. Danny and Derik pointed out that in order to run important algorithms in machine learning, you have to be able to clean and process your data before you use it, which is something I’ve witnessed this summer. Danny also mentioned that software and hardware go hand in hand, so learning more about hardware can improve your software. He showed an example of how he ran his code in parallel to maximize CPU and make his code more efficient. I’ll keep this in mind as I continue developing my tool to see if I can apply these concepts to increase my tool’s efficiency!
                </p>
                <p>
                  Early this afternoon, I connected with Benjamin who is another LEAP intern. He talked about his project, which reads and processes CSV data. He also has a side project that works with Outlook to help people communicate and connect more smoothly. It’s interesting how diverse the intern experience can be even within the technology sector. This is encouraging in that if I am extended an offer for next summer, I may have the opportunity to explore something completely different!
                </p>
                <p>
                  For the rest of the afternoon, I worked on my Spend classification tool and my end-of-internship presentation. My session with techworks seemed to fix my memory allocation issue! I spent a while altering my training and testing data to take advantage of the whole data set given to me. Eventually, I was able to get the commodity code predictions working and printed to a new Excel sheet. Now that I am training on more data, my accuracy issue with Level 0 commodity codes is resolved! All 3 commodity code levels (Level 0, 1, and 2) are at 97-98% accuracy which is great! However, since I am now running 3 prediction models and using a lot more data, my program is much slower. Currently, all the predictions are happening in 1 file, but it might be beneficial to break up the predictions not only for efficiency, but also for usability in case users only want to view predictions for one level. I will work on implementing this tomorrow. I also want to isolate all the rows where the predictions are incorrect and print them into a separate spreadsheet for ease of analysis. Since my code was taking a while to run, I worked on my presentation for next Friday, where all the CTG interns will present what they’ve learned/worked on.
                </p>
                <p>
                  Tomorrow, I plan to continue working on my Spend Classification tool. I will separate predictions into 3 files and isolate the rows where my predictions do not match the actual classification. I also plan to continue working on my presentation if I have downtime. I don’t have any planned meetings tomorrow, but I might reach out to Kiran to double check on the status of getting access to the database.
                </p>
            </div>
          </div>
        
          <div id= "July-14-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 14, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Tuesday</p>
                </div>
                <div class="post-title">
                    <p>Multilevel Commodity Classification</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I spent most of my morning getting the predictions for Level 0, 1, and 2 commodity codes working on 1k rows of data. Since I don’t have the memory allocation errors fixed yet, I’m training on about 3k rows and testing on 1k rows. This allowed me to work through more basic bugs and fix formatting before I move on to larger data sets. It seems like my model predicts Level 1 and Level 2 pretty accurately (97%), but has some trouble with Level 0 predictions (72%). I’m not sure if there is some fundamental flaw with my program or if this is due to a lack of robust training data. Once I fix my memory allocation error and am able to train on more data, I’ll be able to see if I need to go back and fine tune my model to work with Level 0 predictions.
                </p>
                <p>
                  Later this morning, I had my 1:1 with Rajiv. We talked about my current status with the project. I updated him on the information that Kiran gave me yesterday, where I’m stuck, and what I plan to do next. He suggested a method of implementing manual rules, which relies on a field in the database that informs whether or not rules have been defined for a column. He also tried to help me troubleshoot the memory allocation error I was running into. Ultimately, I’ll have to go to techworks to see if they can bypass the administration access or provide me with some other workaround. A few interns in the CTG group were talking about final presentations and Rajiv confirmed that there is a brief presentation where interns talk about what they learned throughout the summer, so I might start preparing for that.
                </p>
                <p>
                  Around noon, there was an info session about the LEAP program, which is meant for new college graduates joining Fidelity as Full-stack Software Engineers, Data Engineers, or Systems Engineers. I’ve heard a lot of great things about the LEAP program from Nancy, Rajiv, and others, so I was excited to learn more. The LEAP program is meant to train new associates and make the onboarding process smoother. Something I found particularly helpful is that you know your manager ahead of time and you can tailor your learning opportunities based on what team you will be on. I might ask Nancy more about her experience with LEAP, since I’d like to know more about LEAP or similar onboarding programs.
                </p>
                <p>
                  In the afternoon, I caught up with Derik. I asked him about how he started at Fidelity and how he got to the position he is working in today. His experience is different from Nancy’s so it was cool to hear a different perspective. He also talked about the workflow his team uses, which is different from our team’s workflow. Given the nature of his projects and his smaller team, they use less of an Agile sprint methodology and instead run longer projects. I wasn’t aware of how new Agile was to Fidelity (introduced 2 years ago) so I was under the impression that most teams were using Agile. It was eye-opening to see how different teams with different needs might use other methodologies that work better for the projects that they are working on. Derek also mentioned that the AI club has some interesting resources and articles, so I might look into some articles to discuss in our next meeting!
                </p>
                <p>
                  I contacted techworks about my memory allocation error, but I think they misunderstood my question, so I’ll try calling again tomorrow to resolve my issue. I also contacted Kiran to clarify what implementation he wanted for manually adding rules and I’m hoping to hear back from him tomorrow. I started thinking about what I might include in my end-of-internship presentation and I might start working on my presentation tomorrow if I have downtime. I’m usually not a big fan of presentations, but I’m eager to share my project with other interns and to learn about what they’ve all been working on.
                </p>
            </div>
          </div>

          <div id= "July-13-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 13, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Monday</p>
                </div>
                <div class="post-title">
                    <p>New Requests</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  This morning started off pretty similar to how I left off last week. After attending the scrum meeting, I spent the rest of my morning completing the machine learning course that I started on Friday. A lot of this course was a good review from the beginning research phases of my project and showed ways that I could apply different concepts. Some new concepts that I learned were about regression and reinforcement learning. These concepts are not directly applicable to my Spend classification project, but I think that these general concepts are good to know. Additionally, Derek set up a meeting with one of his coworkers in data analytics and insights for Wednesday, so I completed a course on data analytics so I will be able to ask more thoughtful questions during our meeting and so I can understand the kind of work he does more easily. Lastly, I completed a course on how to be a better mentee. Mentor relationships are heavily emphasized at Fidelity as being a good way to network and grow in your professional life, so I wanted to learn more about best practices for how to get the most out of my mentor relationships. There are a few other LinkedIn Learning courses that I can complete, but for now I think I’ll take a break from these courses, since they’re getting a bit monotonous.
                </p>
                <p>
                  This afternoon, I caught up with Nancy for our 1:1. I mentioned where I was stuck with my project since I was waiting on responses from a few people. Nancy suggested that in addition to LinkedIn learning, I could also spend my downtime networking or even ask my team if they need any help. I set up a quick networking meeting with Benjamin from our intern group on Nancy’s suggestion. Nancy and I also talked about our group and how we could connect more. After the meeting, I spent some time trying to find games we could play together, and I have a few good options!
                </p>
                <p>
                  This afternoon, I also attended a Virtual Career Showcase for the Emerging Leaders Group (ELG), which is a cohort for onboarding those with liberal arts undergraduate degrees. While I am more interested in joining the LEAP program, I thought it would be interesting to hear about other programs and get an idea of other options that exist. This showcase was pretty interesting and it seems similar in format and length to the LEAP program, but of course the contents are different.
                </p>
                <p>
                  Later in the afternoon, I caught up with Kiran briefly. He mentioned that he brought up my project to Janice and a few members of the India development team. It seems like people are pretty interested in my project! :) This is really exciting to me and I’m glad that people are interested in the work that I am doing. Kiran mentioned that I could consider doing a demonstration of my tool for some members of the India development team later this week if we can find a time that works with everyone’s schedules. Kiran also brought up my project with his boss, who had a few suggestions for where to expand on this project. Previously, Daniel and David mentioned predicting the Level 2 commodity codes, but Kiran’s boss mentioned that it would be helpful if I could also predict the Level 1 and Level 0 commodity codes. These predictions would be printed in adjacent columns to the existing correct data in the spreadsheet or database. This is something that I can work on integrating. Kiran also mentioned that it would be helpful to be able to add manual rules somewhere. This implementation is something that I will have to think about more deeply since it doesn’t work smoothly with my existing model. I may have to add a file for where you can hardcode rules and apply the corresponding commodity codes, but I’m afraid this will harm the efficiency of the project. The last thing Kiran mentioned was that we might be implementing my project with Snowflake, so I might look into taking an introductory Snowflake course.
                </p>
                <p>
                  I worked on expanding the current code for Level 2 commodities to work on Level 1 and Level 0 commodities, but I’m still struggling with some preliminary errors. I plan to continue working on Level 0, 1, and 2 predictions tomorrow and start thinking about how to implement manual prediction rules. I also have my 1:1 with Rajiv tomorrow, where I plan to bring up the memory allocation errors I was running into last week. Fidelity also has its LEAP program showcase tomorrow, where I’m eager to learn more about since I’ve heard so much about it!
                </p>
            </div>
          </div>
        
          <div id= "July-10-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 10, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Friday</p>
                </div>
                <div class="post-title">
                    <p>Databases and Data Science</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  Today was a very laid back day, since I’m still stuck and can't progress with my Spend classification tool. I haven’t yet received a response from Kiran about getting a read-only username and password to the database. If I don’t receive a response by next week, I’ll send a follow up message, but it seems like there are some issues with the databases currently, so it may be a little while before I have access. I am also still stuck with regard to predicting commodity codes since I’m still running into memory allocation errors and do not have administrator access. Rajiv mentioned that he has been busy and will help me as soon as possible. I updated him on the fact that I am learning some new topics using LinkedIn learning in the meantime. The only other aspect that I can work on for the Spend classification is servers, but based on what Don said, it seems like the decision of whether or not to even use a server is something we will need to discuss as a larger group when we get closer to the time of implementation
                </p>
                <p>
                  Since I have a lot of downtime while I wait, I’ve been using LinkedIn learning to learn about a variety of topics. Today I completed a course on databases and a course on data science. The database course taught me some general concepts about database design, use, and basic SQL commands. I think the information from this course will be useful for when I have access to the Oracle database and need to pull Spend data. The data science course taught me more general information about how to explore data, answer hypothesis questions, and the difference between data science and statistics. I also started a general machine learning course, which was informative for learning about different applications of machine learning that I can look into later.
                </p>
                <p>
                  At midday, there was a panel discussion with previous tech interns. We got to hear perspectives from Kyle, who is on his third Fidelity internship, Alejandro, who is currently in the LEAP program, and Revanth, who recently graduated from the LEAP program and is currently working full-time. It was great to hear their experiences and advice surrounding the internship program and the onboarding process.
                </p>
                <p>
                  Today I also reached out to Derick with some generic questions about getting help and moving past roadblocks. Next week, I hope to move forward with the Spend Classification tool. Even if I remain stuck, I have access to plenty of learning resources through LinkedIn learning and I can reach out to Nancy and Derick to see if they have time to give advice about career building and talk about their experiences at Fidelity. There are also a number of intern webinars next week to look forward to, surrounding career showcases, virtual town halls, and elevator pitches.
                </p>
            </div>
          </div>

          <div id= "July-09-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 9, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Thursday</p>
                </div>
                <div class="post-title">
                    <p>Fixing Stopwords</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I had a morning meeting with Nancy today. We caught up on work and I told her about my presentation yesterday. I mentioned that I get a little nervous giving presentations and she offered to help me practice for presentations in the future. If she has free time, I’ll probably take her up on this offer since I think it’ll help me be less nervous about presenting in the future. She also mentioned that as part of the LEAP program, everyone had to give a 20 minute presentation about a topic of their choice. This sounds pretty intimidating, but all of my presentations are academically oriented, so maybe it would be easier or just different to give a presentation about a hobby. Nancy suggested that we could even do a 5-10 minute version of this with our intern group if people aren’t too busy. I also asked Nancy more about her experience with the LEAP program. I’ve been hearing a lot of great things about LEAP and since Nancy took part in the program not too long ago, I thought it would be informative to hear her thoughts. From what she shared, it sounds like a lot of fun! It seems like a great way to get adjusted to the work environment, bond with other associates, and learn new skills. I think that I would definitely consider joining the LEAP program in the future if I was given that opportunity. :)
                </p>
                <p>
                  For the rest of the morning, I worked through some of the issues I was having yesterday with combining stopwords and lemmatization. I ended up directly altering the stopwords file from NLTK to include the lemmatized stopwords. This means that the new stopwords are hardcoded in, which might not be the most sustainable solution, but I wasn't able to find an alternate solution that wouldn’t mess with the efficiency of my existing program. Additionally, excess stop words that aren’t covered by this process shouldn’t be too big of a deal. Since I’m using TF-IDF, stopwords likely have a high document frequency, so they shouldn’t cause too many errors with the categorization. I reimplemented lemmatization into the commodity prediction program and it seems to be running smoothly.
                </p>
                <p>
                  In the afternoon, there was a webinar about Diversity and Inclusion. It was eye-opening and inspiring to hear about the experiences of associates from minority groups, specifically Black associates, in light of recent events and the discussions being raised about racial inequality after George Floyd’s murder. Associates discussed their experiences with microaggressions in the workplace, how they raised their voices to speak against injustices in their personal and professional lives, and inspired us to take action against racial injustice.
                </p>
                <p>
                  Throughout the rest of the day, I continued trying to work around the memory errors I was getting regarding allocating memory for a large array. There were a few times where the code ran successfully for 100k rows, and I got an accuracy of about 98% which is great! However, it doesn’t run without error consistently for anything greater than 50k rows. After some online research, it seems like I could override this error by changing some file size settings. However, in order to change these settings, I need administrator access. I messaged Rajiv about this error and asked if there was any way to get an admin to bypass these settings, or if it would be better for me to just stick with training on 50k rows even if it is less accurate. It seems like Rajiv was stuck in back-to-back meetings today, so I wasn’t able to get a response today. In my downtime, I decided to test how accurately the model could predict with 50k rows of training data. On 100k rows, the model is 93% accurate and on all 235k rows the model is only 74% accurate. So as predicted, more training data definitely leads to better results.
                </p>
                <p>
                  During my downtime in the afternoon, I also watched a course comparing Waterfall and Agile frameworks. I think this helped me understand more of the benefits of using Agile. I’m hoping to get access to the database via Kiran or an administrator settings override via Rajiv in the near future, but I understand that they are busy. I’ll ask Rajiv if he has suggestions for other tasks I can undertake or other areas for me to look into while I am waiting on these components to move forward with my tool. Otherwise, I can look into more career development courses on LinkedIn learning via Fidelity central. At the end of the work day, I came up with a question for Derick about his decision to get an MBA, which I plan to message him about tomorrow. I also made a collage of my interests for an ice breaker for my mentor group chat with Nancy in the afternoon. I hope to make progress with the Spend classification soon, but in the meantime I can busy myself with networking and career development!
                </p>
            </div>
          </div>

          <div id= "July-08-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 8, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Wednesday</p>
                </div>
                <div class="post-title">
                    <p>Update Meeting!</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  This morning there was no scrum call since the whole APT team was gathered for a Bi-Monthly team meeting. Janice led the meeting and there were plenty of new faces. A couple of teams talked about the broad overviews of their projects and what they’re currently working on. Rajiv and Ramya from our team explained the database-related work that our team is doing. It was interesting to see how this larger meeting differed from the usual scrum calls, which are much smaller and much more detailed. This call discussed longer-term plans, was more like a presentation than a discussion, and had a lot more people.
                </p>
                <p>
                  Today I also had the opportunity to explain my progress to some familiar faces from the business team. David, Cameron, Daniel, Kiran, Rajiv, and Mallikarjuna were present at this meeting. Kiran and Rajiv were already pretty much caught up from Monday’s meeting, but this was the first time that David, Cameron, and Daniel were able to see the code that I had been writing. Kiran invited Mallikarjuna onto the meeting and it was nice to have a fresh perspective. Since the last meeting was when I was still in the initial research phase of the project, I covered pretty much all the code, algorithms, and applications of the model. Based on Kiran’s suggestions from Monday, I started off by explaining the high-level concepts of how my tool worked and explained that I was using a widely-known machine learning algorithm. I then walked through the basic outline of my code, with more focus on TF-IDF since this was how the model was making its own ‘rules’ about how to categorize data. From here, I showed how I applied the model to make predictions for defined purchase codes and compared these predictions to the actual purchase codes. At this point, I answered a couple of questions about TF-IDF and LinearSVC. I then moved on to explain how my code made predictions for unspecified purchase codes. David brought up that I could also look into predicting a different field called commodity codes using purchase codes as part of the training description. Since the sample data I currently have does not include the commodity codes and I don’t yet have access to the database, David and Daniel offered to send me an updated spreadsheet with commodity codes until I had access to the spreadsheet. Kiran and Rajiv also brought up how we were working reading directly from a database and using a server. We discussed these points for a little while, but it seems like we’ll discuss this in more detail as the tool becomes more advanced and more applicable. Overall, it seems like I’m headed in the right direction with what I’ve done so far! It doesn’t seem like the accuracy or efficiency of the tool need to be improved very much. The next steps are to alter my code to make predictions for commodity codes and once I have access to the database, I’ll be able to work on reading from the database directly.
                </p>
                <p>
                  I also had a meeting with my new FYPN mentor Derick today. We mostly talked through introductions about my previous CS experience, his work history at Fidelity, and his academic history. I mentioned that I was interested in AI and he offered to reach out to some team members to see if I could shadow or get connected with them. He also told me about some interesting ways that Fidelity is using robotics. Since he’s more involved with data science and he mentioned that he got an MBA, I am excited to ask him more about his career path and choices, since I’m considering getting an MBA at some point. We have weekly meetings set up, but I hope to message him between meetings as well.
                </p>
                <p>
                  For the rest of the day, I worked on predicting commodity codes as David suggested. For now, I’ve removed the lemmatization steps since it was causing some warnings with the stop words. Once I have a prototype that can successfully predict commodity codes, I plan to work through these smaller errors with text preprocessing. I made a few alterations to my purchase code prediction program, but I’m receiving errors about being unable to allocate memory for my data. :( It seems like my data is too large to convert to an array with the added commodity code column. I can train my model on 50k rows, but I run into errors for anything larger. I’m still working through these errors, but I might need to reach out to get help or override some settings to truly resolve them.
                </p>
                <p>
                  Tomorrow, I have my 1:1 with Nancy, where I plan to ask about mobility, getting placed on teams, and adjusting to new teams. There’s also a webinar about Diversity and Inclusion for interns in the afternoon which I plan to attend. I might reach out to Derick if I come up with any questions throughout the day about his work. Otherwise, I’ll continue to work on getting the commodity code predictions and hopefully I’ll be able to work around the memory errors.
                </p>
            </div>
          </div>

          <div id= "July-07-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 7, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Tuesday</p>
                </div>
                <div class="post-title">
                    <p>Meeting Prep And cx_Oracle</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I started my morning by reaching out to Himanshu to ask for his advice on reading information from oracle databases using python. He recommended that I import the cx_Oracle package and gave me a lot of great information about how to set up an environment and proxy to access a database. He provided me with some of the code he used so that I had a starting point for what information I would need in order to read information from the database. He also showed me an example of how he processed information from the database by iterating through different rows. Today I also had my 1:1 meeting with Rajiv. Since I updated him and Kiran on my project yesterday afternoon, I didn’t have anything new to report today since he was already up-to-date. I mentioned that Himanshu had given me a lot of great advice for working with databases and Rajiv gave me some additional insights. He demonstrated what kinds of commands can pull information directly from a database and export the information as an Excel sheet, which is what I have been working with. He suggested that I might be able to just create an intermediary step to turn the information from the database into an Excel sheet and then my existing code would work just as it does now. He also explained what the different pieces of information that I need to access a database mean in context so that they don’t seem like such abstract concepts.
                </p>
                <p>
                  This morning I also attended the team’s weekly update meeting. As per usual, I had a general understanding of what was being discussed in terms of dropping, inspecting, and changing components, but I wasn’t able to follow the specifics. Something interesting about this meeting was that there was a lot of back-and-forth discussion about the best procedure to address a task. I thought it was interesting how some approaches favored completing tasks as quickly as possible, while other approaches favored a more consistent pace to account for people’s bandwidths and someone brought up that being too ahead of schedule might mean that the project could grow in scope from what was originally agreed upon. Listening to the discussion gave me a better understanding of how different people have different styles of working and how it's important to come to a consensus or a compromise as a team.
                </p>
                <p>
                  I messaged Kiran to ask for some of the information that I would need to access and connect to the database after my meeting with Rajiv. It seems like Kiran is quite busy as he has a lot of back-to-back meetings, so I didn’t receive a response today, but I can ask again during our larger meeting tomorrow. Connecting to the database is not an urgent matter, so I spent some time on my personal career development while I was waiting. I watched some online courses about office politics and how to succeed in a new job. I think I can apply a lot of these concepts to this internship as I move forward, to my next internship, and to my career in the future. These courses highlighted the importance of professional relationships and finding mentors. As a matter of fact, I applied for another mentor through FYPN last week, specifically in the field of AI. Today my mentor, Derick, reached out to me and set up an introductory meeting for tomorrow! I compiled a few questions to ask him during our meeting about his work, how Fidelity uses AI, and how he got involved with AI. Since my project this summer is concerned with NLP I am interested in hearing more about applications of AI and how he decided on a career in AI.
                </p>
                <p>
                  This afternoon, there was a webinar about managing money from Personal Investing specifically for interns. We covered budgeting, credit, debt, investing, and retirement. It was nice to get some advice specifically directed toward young people and I can definitely apply what I learned to my personal finances. I really like that Fidelity has specifically created programs to help not only customers but also associates with their finances. :)
                </p>
                <p>
                  For the rest of the afternoon I worked on preparing for my meeting tomorrow with the Business team. I feel pretty prepared, but I’m just hoping to cover all the points I want to and truly showcase what I’ve been able to accomplish in the last few weeks. I’m definitely ready for some new feedback and an idea of what steps I can take to expand the project to beyond what I’ve done so far. I’m also excited to meet my new FYPN mentor Derick tomorrow!
                </p>
            </div>
          </div>

          <div id= "July-06-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 6, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Monday</p>
                </div>
                <div class="post-title">
                    <p>Servers and Lemmatization</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  This morning I had my meeting with Don to discuss the possibility of using a server for my project. Don mentioned that I should consider a few items before jumping into using a server, namely the scope/computational power of the project, how many people will be using the application, and how much maintenance is needed. He gave two examples of servers: VM and C2C. A VM server is very powerful and does not require much maintenance, but it takes a few months to get approved, which might not be viable for my project. Additionally, since my project would not be used enterprise-wide a VM server would be overkill. A C2C server only takes a few hours to get approved and would be more appropriate for the scope of my project, but it would require a lot more maintenance. I think this is something I will need to discuss more with Rajiv and Kiran before deciding if a server is really necessary for this project. A more likely solution is that I’ll find a way to get my program onto the desktops of those who will be using my tool. Don also talked to me about the LEAP program for newly hired associates Fidelity and how this allows for accelerated career growth and an easier transition in Fidelity. I’ve heard a lot of great things about Fidelity’s LEAP program and Fidelity’s work culture as a whole and it’s given me a lot to think about when it comes to thinking about my future career path.
                </p>
                <p>
                  I also worked on a new format for writing outputs today, where the newly predicted purchase codes can be written directly into the corresponding column of an existing spreadsheet. I’m still not sure what the format the final product will be interacting with, but if it is contained in an Excel spreadsheet like the sample that I was sent, then this would be a good feature to have implemented. Otherwise, I can work with the new format once I get confirmation from the Business team.
                </p>
                <p>
                  Today I also worked on implementing lemmatization as part of the text preprocessing. Lemmatization allows different forms of a word to be grouped together and analyzed as a single item. For example, run, running, and ran would all be grouped together using lemmatization. Previously, I was relying solely on the TF-IDF system for transforming the descriptions into vectors to be used in the LinearSVC classification model. However, I remembered that lemmatization was mentioned in the article that David sent over, so I thought I might try and implement this as part of the text preprocessing before being transformed using TF-IDF to see if it would further increase my accuracy. Ultimately, it seems like it increases the accuracy of my largest 200k dataset by .12% which doesn’t seem like much, but this could amount to hundreds of rows in the long run. It doesn’t seem to positively impact the accuracy of smaller data sets as much, but I think that since Spend data is generally larger data sets, I will keep this lemmatization implementation. I think lemmatization will prove to be a more helpful feature if I am given more data to train my model, since it works better with more training data
                </p>
                <p>
                  Today I also updated Rajiv and Kiran on my project. It seems like accuracy (95-96%) and efficiency (2-3 minutes for 200k rows) of my tool so far is pretty good, so I don’t need to worry too much about researching new ways to increase accuracy and efficiency. They also mentioned that the Business team will likely be most interested in the feature where my model is able to make predictions for the unspecified purchase codes, since this is what they are having the most trouble with. Kiran was also able to take a quick look at the predictions my model was making for the unspecified data and confirm that they were in the correct general categories. I’ll get more feedback on the accuracy of these predictions from the Business team on Wednesday. Rjiv and Kiran also recommended that I start looking into how I might read information from a database instead of an Excel spreadsheet. Rajiv mentioned that Himanshu has experience in this area, so I might reach out to him and ask if he has any recommendations or insight for the best packages or methods for this. Lastly, Kiran recommended that I be prepared to explain some background information on my code and algorithms to the business team on Wednesday so that they understand the process used for categorizing the data.
                </p>
                <p>
                  Tomorrow, I’ll reach out to Himanshu about reading information from databases. I’ll also put together a list of topics I need to cover, questions I have, and demonstrations to prepare for my meeting with the Business team on Wednesday to make sure that I hit all the points that I need to and that I don’t forget any of the questions that have come up. Otherwise, I’m pretty happy with where I am with this tool in terms of presenting an update to the Business team!
                </p>
            </div>
          </div>

          <div id= "July-02-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 2, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Thursday</p>
                </div>
                <div class="post-title">
                    <p>Agile Frameworks</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  This morning the scrum meeting was cancelled, so I was able to finish the LinkedIn learning course about servers (that I started yesterday) earlier than expected. Through this course, I have been able to come up with more concrete questions that I can ask Don during our meeting. Unfortunately, something came up during the afternoon for Don, so I rescheduled our meeting for Monday morning. I’m planning to ask questions about what kind of server setup I should consider (Microsoft server?), what role of server I might need (file, hyper-v, document?), and how I can configure an IP address.
                </p>
                <p>
                  For the rest of the morning I worked on increasing the efficiency of my Spend classification tool by saving and loading the model. The original code was both training and writing predictions each time I ran the program. I thought it might be more efficient if I could save the trained model so that only the predictions would need to be calculated on run since the model should be trained the same way each time. There are a couple of ways to save a model, but the two that I researched were pickle and joblib. Ultimately, I decided to go with joblib, since it's better suited for larger arrays. It was pretty straightforward to implement joblib and save the trained model to a new .sav file. There were a couple of steps relating to loading in variables from the training file before being able to make new predictions in a new file, but overall it was a pretty smooth transition. With this implementation, I can train the model once on a large amount of data to get the most accurate results. Then, when needed, the program will have a separate file to write predictions based on the already-trained model more efficiently. I think this should aid with the previous problem with sacrificing accuracy for efficiency or vice versa so that we get the best of both worlds. :) In terms of runtime, the old version of the program took 3:30 minutes to write predictions for 200k rows. The new version takes 1:30 minutes to write these same predictions. This is a pretty significant efficiency boost and hopefully this will make the tool more scalable!
                </p>
                <p>
                  Today I also got to take a virtual tour of FCAT/Innovation Center. I learned about how FCAT works with new emerging technologies and builds tools to help Fidelity associates and customers succeed. There was a lot of emphasis on touchless interaction and VR technology. I’d be interested in exploring some of the areas mentioned during this tour, which seem to be increasing in importance, like cybersecurity and AI. I have some exposure to AI this summer with NLP for Spend classification, but I would be interested in being more immersed with AI and exploring other applications.
                </p>
                <p>
                  For the rest of the afternoon, I learned about different types of Agile frameworks. I found a course yesterday that compared DA (Disciplined Agile) and Spotify Agile and explained which framework might be best suited for different enterprises since Rajiv mentioned how Fidelity is moving toward a Spotify Agile framework. After completing this course, I found another course on the basics of Scrum, which is what my team currently uses. I was able to get through most of the course today and I plan to finish the last piece on Monday. Since Scrum is widely used not only in Fidelity but in many organizations, I’m excited to be able to apply what I’ve learned in the short-term and long-term future.
                </p>
                <p>
                  I have tomorrow off for Independence Day, but I’m excited to come back on Monday and get some feedback from Don about how to use servers and from Kiran about my progress with the Spend tool. I also have my meeting with Nancy on Monday, where I might ask more about FCAT or about how mobility works within Fidelity, especially in tech. Next week I get to show my progress to some of the business team as well, so I’m excited to hear their feedback and move forward!
                </p>
            </div>
          </div>

          <div id= "July-01-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JULY 1, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Wednesday</p>
                </div>
                <div class="post-title">
                    <p>Learning About Servers</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I spent a majority of my day today starting to learn some basic information about servers. Since I am meeting with Don tomorrow to discuss the possibility of using a server to allow other associates to use and improve the tool that I have started working on, I wanted to get familiar with some terminology and basic components of servers. I started off with some Youtube videos explaining what a server is and the basic applications of why you might want to use a server. I then moved on to a LinkedIn learning course which was accessible through Fidelity central about managing and administering a server. Today I was able to complete 5/7 of the lessons in the course and I plan to finish the remaining 2 lessons tomorrow before my meeting. Some of the information in the course may not be directly related to my use of a server, but I think it’s still interesting and good information to have.
                </p>
                <p>
                  Today I also had my meeting with Nancy. Most of the time I asked about her academic/career journey that led her to where she is. A lot of the time, when professors or other associates talk about their careers, they have been working for a number of years, so their advice is generally about finding a passion and pursuing it. I think that since Nancy is closer in age to me, a lot of her advice was more directly applicable. She talked about how she had switched majors many times before declaring and how she is still solidifying her career goals. She also recommended some resources for me to look into. In particular she recommended looking into FCAT, which is at the forefront of innovation for Fidelity. I booked a virtual tour for tomorrow afternoon to check out what FCAT does and if this is something I might be interested in applying for next year or doing more research on. 
                </p>
                <p>
                  I also had my 1:1 with Rajiv today, where we discussed my progress with the classification tool. I explained what I had accomplished with increasing the accuracy, printing the outputs, and deciding on an algorithm. I also explained that some feedback on where I am might be useful to making sure I am on the right path. Rajiv recommended that I set up a meeting with him and Kiran for Monday next week to update him and get advice on any last-minute changes and then to set up a meeting with the Business team for early next week. I have set up both meetings and am excited to get some new feedback!
                </p>
                <p>
                  Rajiv also told me a bit more about agile, sprints, and workflow. I learned more about how the pre-planning and planning stages affect the sprint pacing and how new unexpected tasks can affect this. I also learned about spotify agile, which is an up-and-coming methodology similar to the version of agile used within Fidelity. Rajiv recommended that I could look into spotify agile if I have free time, since this seems like a newly popular method. I found a LinkedIn learning course for agile vs spotify agile that I plan to go through if I find that I have free time. :)
                </p>
                <p>
                  I expect tomorrow to be similar to today, where I’ll finish up my course about servers, take my Virtual FCAT tour, meet with Don, and perhaps learn about spotify agile if I have time. Looking forward to learning more about servers and how I might use them!
                </p>
            </div>
          </div>

          <div id= "June-30-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 30, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Tuesday</p>
                </div>
                <div class="post-title">
                    <p>Parameter Fine-Tuning</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I started my morning with 3 back-to-back meetings with the FPAIT APT team. We started with the usual daily scrum call, where I am  able to follow along but I’m still not familiar with the specifics. I think that unless I get directly involved with the database system it will be difficult for me to understand the ins and outs of the daily scrum calls, but it’s still nice to check and see what the team is up to. Next, there was an end-of-month meeting discussing what was accomplished in June. This gave a broad view of what the team had accomplished during the month and what still needed work in July. One part of the meeting that stuck out to me was at the end of the meeting when we had a couple minutes to “shout out a coworker” which was when team members could praise others for exceptional work. I thought this was a nice way to celebrate team members and to break up an otherwise formal meeting especially because the current remote work environment can make it hard to feel connected to other associates. Lastly, there was a meeting about the current update. After last week’s meeting with Poornima, I had a better understanding of what the update was about and what aspects were being discussed, but again I’m not too familiar with the specifics of the update components.
                </p>
                <p>
                  I spent the remainder of my morning and early afternoon exploring parameters for LinearSVC. Since I decided on LinearSVC as my algorithm of choice for today, I wanted to experiment with changing the default values of the parameters to see if I could improve the accuracy at all. I found that the parameters that maximize output vary depending on the size of the data set. I initially experimented with values for my 2k row data set, but when I applied the same parameters to the total 200k data set, it actually decreased the accuracy. Ultimately, I found that the variables that affect the accuracy of my data are C, fit_intercept, and intercept_scaling. Other variables like tolerance, verbose, and max_iter generally don’t affect the accuracy or the default value consistently produces the most accurate results. Ultimately, changing the parameters didn’t make a huge difference, since it just increased my accuracy from 95% to 96%, but on a larger scale this small percentage change could be a difference of hundreds or thousands of values being correctly categorized. I’m pretty happy with the progress I’ve made for now. :)
                </p>
                <p>
                  This afternoon there was a ‘Lunch and Learn’ panel with Personal Investing and how they’ve been responding to market volatility and Covid-19. Since my project and my team are not directly involved with the markets or investing, it was interesting to learn about a completely different field under the same company. I think I’ll continue to make an effort to attend a variety of ERG events since it seems like a good way to get to know what’s happening in other branches of Fidelity.
                </p>
                <p>
                  Today I also got involved with the Digital Champions program in the intern group, which is designed to help promote engagement, plan events, be a technological leader, and give real-time feedback. There are a couple options for how to get involved, and I think I can realistically commit to the 21-Day challenge which has daily 5-10 minute tasks and the Elite 21-Day challenge which has daily 10-20 minute tasks. The other more intense programs might not be realistic for my current workload, but if I find myself with free time I’ll definitely look into them! There were a couple of quick tasks today about navigating Microsoft Teams and Outlook and I’m excited to see what else is in store.
                </p>
                <p>
                  For the rest of the afternoon, I looked into different methods for vectorizing text. I am currently using TF-IDF (Term Frequency-Inverse Document Frequency) which pulls out the most ‘important’ words for each category in a document based on its frequency of appearance for a category compared to the entire document. I’ve come across other vectorization methods like word2vec and gloVe during my research phase in the first week and in the article that David sent. Ultimately, it seems like TF-IDF works best if you have a large labeled data set (which I do). Word2vec and gloVe are strongly favorable only if you have sparse training data, but when you have >10k rows of training data, TD-IDF outperforms other methods, so I will stick with TF-IDF. I also experimented with TD-IDF's parameters to see if I could further increase the accuracy, but it seems as though the values I have been using are optimal. For now I think I’m done experimenting with parameters to increase accuracy, but I’ll see if Rajiv has further suggestions for how to improve accuracy and/or efficiency tomorrow.
                </p>
                <p>
                  It seems like the end of the month is a busy time for a lot of people! Rajiv rescheduled our meeting for tomorrow morning, so I look forward to updating him on my findings and my progress then. I also have my meeting with Nancy tomorrow, where I’ll ask the questions that I compiled yesterday. Since I’m happy with my progress with my classification tool for now, I plan to spend my time tomorrow looking into servers. I don’t have prior experience setting up or using servers, so I’d like to learn more before my meeting with Don on Thursday so that the meeting is as productive as possible and I can follow along with any suggestions he has without needing to ask for clarification on the more basic terms.
                </p>
            </div>
          </div>

          <div id= "June-29-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 29, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Monday</p>
                </div>
                <div class="post-title">
                    <p>Deciding On LinearSVC</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  This morning, after the scrum meeting I fixed my method of random sampling. Last week, some of the data that I was writing prediction outputs for was duplicated from the training data, which meant that my results were misleadingly high. I fixed my method so that I removed some data randomly using the .sample() and .drop() functions at the beginning before the model was trained to save for testing and writing predictions. This allowed for more realistic results. For a set of 200 rows, the model was about 70% accurate and for a set of all 200k rows, the model was about 95% accurate. This matches the accuracy levels seen last week for Linear SVC when comparing algorithms. There is still quite some work to do for fine-tuning the model to be even more accurate.
                </p>
                <p>
                  There was also a panel with some of the executive leadership at Fidelity to answer questions from interns. We got to hear from Abby Johnson (President and CEO) and members of her senior leadership team on topics like current events, advice to their younger selves, professional growth, business strategies, etc. It seemed like the members of the senior leadership team were pretty close and made conversation and jokes throughout the webinar, which was a pleasant surprise since a lot of the webinars so far have been pretty strictly following a pre-set schedule. A lot of the leadership also mentioned their experience working at companies prior to joining Fidelity and I think that these were often the most insightful topics. A lot of the advice they gave had to do with taking risks and opportunities when possible and not ‘sweating the small stuff’ during college and the beginning of your career. This was nice to hear, especially coming from a group of people who have achieved such great things in their careers. Hearing about leadership roles from this panel definitely got me thinking about more managerial roles and job opportunities in my future.
                </p>
                <p>
                  Today I also got in touch with Don from the System Engineering team about setting up a Fidelity server. I gave him a brief overview of my project and the scalability concerns I was having and asked if he had any insights. However, since setting up a server is a pretty technical matter, he asked to set up a meeting with Rajiv and I to talk through some of the more technical aspects before offering advice or recommendations. It seems like both Rajiv and Don are pretty busy this week, so the first reasonable availability that I could find was for Thursday afternoon. I’m looking forward to working towards some of the next steps and looking into how I might be able to scale my tool. I’ve never worked with servers before, so this is another new field that I can look into this summer.
                </p>
                <p>
                  For the rest of the afternoon I worked more on my code and prepared what I have so far for my 1:1 with Rajiv tomorrow and for the business procurement team at some point in the future if needed. I decided to take down the times for the algorithms that I was initially considering so that I had a clear reason for choosing LinearSVC. I went back to RandomForestClassifier and decided to try some other parameters, since LinearSVC and RF have similar accuracy levels. I remember reading about how having a large number of trees can greatly impact the run time, so I tried RF on the original 200 trees, then cut it to 100, then down to 25. At 25 trees, RF takes about 3:30 minutes to run, which is a lot faster than the original 25 minutes to run at 200 trees. However, this is still considerably slower than LinearSVC which only takes 1:45 minutes to run. I then ran SGDClassifier, which had a comparable runtime to LinearSVC on smaller samples of about 200 rows, but when I increased the sample size it was even slower than LinearSVC. According to quite a few sources, LinearSVC seems to be more suitable to larger scale samples than SGDClassifier. With this analysis done, I feel solid in my choice of LinearSVC for the model’s algorithm in order to maximize both accuracy and efficiency.
                </p>
                <p>
                  I’m still looking into LinearSVC parameters, but I haven’t come up with anything that seems to fit the data that I am using. I may need to rework the structure of my data or keep looking for algorithms to get a meaningful increase in accuracy while maintaining the current level of efficiency. For now, I’m going to keep looking, but reworking the structure of the data is an option that I will keep in mind. I also finally had the opportunity to look into the source that David recommended last week. A lot of the information confirms the choices I have made with SVC and text classification, but it also gave me some new things to think about like using word2vec instead of tf-idf or looking into more deep learning/unsupervised options. I’ll look into these more during my free time and see if I can feasibly work these aspects into my existing structure.
                </p>
                <p>
                  I was scheduled to have my 1:1 with Nancy today, so I compiled some questions about career focus and mobility in tech, but this seems to be a busy week for her, so we’ve rescheduled for Wednesday morning. I also compiled some notes about the broad view of what I’ve accomplished so far, demonstrations of outputs, and further questions for my 1:1 with Rajiv tomorrow. I’ll also continue looking into ways to expand my code using the topics in David’s article or LinearSVC parameters tomorrow. I’m looking forward to meeting with Rajiv to see if he has feedback for next steps I can take.
                </p>
            </div>
          </div>

          <div id= "June-26-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 26, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Friday</p>
                </div>
                <div class="post-title">
                    <p>Ready For Next Steps</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I didn’t have any meetings today, so I was able to focus on my code and gain some insight on where to go next. This morning, I worked on formatting my outputs. I decided to have separate spreadsheets for the ‘UNSPECIFIED’ predicted outputs and comparing the predictions for the randomly sampled defined output to their actual values. I also included some conditional formatting in the defined output page to more easily spot where my predictions did not match the actual values. I decided to run both the ‘UNSPECIFIED’ code and the defined prediction code on 200, 2000, 20k, and the total 200k data set to see how the predictions improved as the data set got larger. It’s a bit hard to make concrete observations on the ‘UNSPECIFIED’ data but it seems like the data is better categorized for a larger data set. However, there are definitely still misclassifications. For example, there are a couple instances where software gets classified as hardware. For the defined outputs, the predictions are definitely better with a larger data set. A data set of 200 is about 93% accurate and a data set of 200k is about 98% accurate. However, with my current method of ‘random sampling’ some of this data is the training data, so the results are misleadingly high. Next week I may look into asking for more data or splitting off a section of the sample data specifically for testing, which would not be included in the training data.
                </p>
                <p>
                  I also captured some visualizations of where the ‘UNSPECIFIED’ predictions were getting categorized. I wanted to see if this matched the distribution of the defined data set to see if the imbalance of the training data set was affecting the predictions. From what I can tell, it doesn’t seem like the imbalance is affecting the predictions too much, but if incorrect predictions into the same categories continue, I will look further into this.
                </p>
                <p>
                  In the afternoon, I worked on improving the accuracy of my code. I looked into the parameters for LinearSVC, but it doesn’t seem like I can feasibly implement a hierarchy system for classifying the data without sacrificing a lot of efficiency. While looking into parameters I stumbled across a method called SGDClassifier, which is for Stochastic Gradient Descent. This uses the same logic as Linear SVMs but using gradient descent to find the optimal parameters. I implemented a version of SGDClassifier using its default parameters and it seems to be almost equally efficient compared to LinearSVC. I’ll need to look more into how I can possibly change the parameters for both LinearSVC and SGDClassifier to improve the accuracy. 
                </p>
                <p>
                  I think that I currently have a pretty efficient and accurate prototype, where it is 95% accurate on a 35,000 row training data set and can make predictions for 200,000 rows in just over 2 minutes. Of course, these can both have room for improvement, but I would like to check in and get some feedback on where I am before pouring time into small accuracy and efficiency changes, in case I need to make any larger adjustments. I might ask Rajiv or Kiran what they think on Monday. Also, since I have the output method set up, I’m wondering if I should prepare to show the business team where I am so far or if I should make more progress regarding scalability before I demo my code to them. I’ll ask Rajiv about this on Monday. Since the machine that Himanshu used to create his server was decommissioned, I needed new resources for setting up a server. After asking Rajiv for additional resources, he directed me to Don, a Systems Engineer. I haven’t had the chance to send out a message to him yet, but I plan to do this first thing Monday morning.
                </p>
                <p>
                  I’m honestly a bit surprised at the progress that I’ve been able to make this week and having great connections like Kiran, Rajiv, David, and Nancy has definitely helped a lot. As I get further into the project, I anticipate that there will be less robust resources and more challenges, since I am still in the initial building phase of the project so far. I’m looking forward to learning more about servers and showing others the progress that I’ve made. :)
                </p>
            </div>
          </div>
          
          <div id= "June-25-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 25, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Thursday</p>
                </div>
                <div class="post-title">
                    <p>Writing Outputs</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I’m definitely getting more used to scrum calls :). This morning, I understood a lot more of the terminology and was able to follow along with the general flow of the meeting. However, I am still a bit lost during the more in-depth discussion of certain components. As I listen in to the calls and get more familiar with the components hopefully I will be able to understand more of these in-depth conversations as well, but it’s nice not to feel completely lost anymore.
                </p>
                <p>
                  This morning I worked on choosing an algorithm to stick with. As I mentioned yesterday, I split LinearSVC and RandomForestClassifier into separate documents so that could compare their speeds. LinearSVC is definitively faster than RandomForestClassifier, so I will be using LinearSVC as my algorithm for finding predictions from now on. I also captured the images of data visualizations from yesterday’s code in case I want to view them or display them during meetings so that I don’t have to re-run my code. It’s actually pretty astounding looking at the graphs and seeing how much of the sample data is unspecified! (~70%).
                </p>
                <p>
                  In the afternoon there was a 2 hour workshop about the economics of Fidelity. This workshop was meant especially for interns to learn more about the inner financial workings of how Fidelity operates. Some of the content was familiar from the Financial Economics and Macroeconomics courses that I took in school, but there was also a lot to learn. Especially interesting to me was seeing how the concepts that I learned in Economics classes were applicable to running a business and investing in different assets. My particular project isn’t really involved with investing, but I think that it’s important to know how other parts of Fidelity work even if I am not directly involved in them.
                </p>
                <p>
                  For the rest of the day, I worked on how to write output predictions into Excel. This process was a bit frustrating, since I found that a lot of sources were outdated or certain functions were no longer functional. Eventually, after quite a lot of trial and error, I was able to successfully get my predictions correctly formatted into an Excel sheet. I first thought it would be interesting to make predictions for the ‘UNSPECIFIED’ items, since these items were cleaned out from the original data. Since these items were previously classified as ‘UNSPECIFIED’ I don’t have a way to cross-reference and make sure that these items have been put into the correct category. When I looked at the data, it seemed like most of the categorizations were not wildly inaccurate. However, since I have only been working with this data for about 2 weeks, I don’t have the intuition for where the data should really be classified, so I will check with Kiran or the Business team. During Tuesday’s meeting with Rajiv and Kiran, they mentioned that it might be good to print out the actual vs. the predicted test data, so I spent a little while tweaking my code to have this functionality. Looking at this data, it seems like my code does a pretty good job, but there are some instances where the categorization is completely different than what it should be. I will need to brainstorm and research ways to fix this issue. Currently, I am thinking about a hierarchy or weighting, since some categories are more relevant to classification than others. But I am not sure how feasible this method would be with the algorithm I am using.
                </p>
                <p>
                  Tomorrow, I have almost no meetings! I plan to look into improving the accuracy of my classification tool as mentioned above. Today I spent a lot of time getting my code to function, but I have a lot of random variables or irrelevant comments floating around, so I will also spend some time cleaning up my code and making it more readable tomorrow. If I have time or if I get stuck, I might also start googling or asking around about setting up a Fidelity server and making my tool scalable like Rajiv mentioned. If possible, I will look into getting my code to be more efficient, but after today, I think some of the speed issues were from running so many models at once, so it may not be as big of an issue as I originally thought.
                </p>
            </div>
          </div>

          <div id= "June-24-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 24, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Wednesday</p>
                </div>
                <div class="post-title">
                    <p>More Accurate But Less Accurate</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I signed on a bit earlier this morning and got in touch with Himanshu. He told me that he used to have the Fidelity local set up, but it is currently decommissioned since the machine they were running it on got decommissioned. This is unfortunate, but I think that further down the line, when I have a better prototype, I can look into more resources and see if there are viable options about Fidelity servers. Perhaps I can reach out to my mentor group, especially since Nancy has added more developers to the chat! Himanshu also suggested that I use Spyder instead of Jupyter within the Anaconda suite, since Spyder is better suited for sunning scripts and applications and is more GUI centric so it would be better for my classification tool.
                </p>
                <p>
                  I sat in on the scrum call this morning and things were a lot clearer after talking to Rajiv and Poornima yesterday! I still don’t understand the details of what is being discussed, but I can follow along and get a general idea of what the conversations are about. I heard a few new terms, but I was able to google them after the meeting and answer those questions.
                </p>
                <p>
                  For the rest of the morning and early afternoon, I worked on transferring my code from Jupyter to Spyder. This went pretty smoothly, since they both run Python. I also ran my code on a larger sample size of 20,000 rows. This was pretty slow to run even after I removed some of the graphics being created. Something I will work on tomorrow is creating separate files for running the Linear SVC and the RandomForestClassifier algorithms so that I can evaluate if either of these algorithms is significantly faster than the other. My current code runs all 4 algorithms (Linear SVC, MultinomialNB, Logistic Regression, RandomForestClassifier) in one file, which could be what is making the program so slow. In larger data sets, Linear SVC and RandomForestClassifier are pretty close in accuracy, so if there is a large discrepancy between the algorithms in speed then this will be the determining factor in which algorithm I ultimately choose to run with. The results of running my program on 20,000 rows is promising! I got up to about 97% accuracy, which I am very happy with. :) However, the code for 20,000 rows takes about 6 minutes to run, which I am not happy about. :( 
                </p>
                <p>
                  When examining the graphics for the 20,000 row data set, I noticed that a very large proportion (about 60%) of the data is classified as ‘UNSPECIFIED’ which could throw off the data by a significant amount. I added a few lines of code to filter out all the data which was ‘UNSPECIFIED’ in order to get more representative results. Doing this cleaning brought the accuracy for 20,000 rows down from 97% to 93%. While the accuracy is technically lower, the previous version 97% accurate tool was ‘classifying’ some data as ‘UNSPECIFIED’ which is not very helpful for a classifying tool. So even if the accuracy score is technically lower, the fact that data is actually being classified correctly 93% is still great for now! For fun, I tried running my code on the total 200,000 row sample set. This took a very long time to run (40+ minutes), but I was curious to see how much data was actually unclassified and how accurate I could get. It turns out that out of about 200,000 rows, about 150,000 rows were ‘UNSPECIFIED’ which was really surprising. I was able to get an accuracy level of about 95%. While I was waiting for my code to run, I looked up some more in-depth information about some of the pieces being used within the program such as TF-IDF (Term Frequency, Inverse Document Frequency) vectorization for turning text into vectors to feed into the algorithms and more information on algorithms like Logistic Regression that I am not as familiar with. I added some comments to my code as well and fixed up the formatting.
                </p>
                <p>
                  This afternoon, I also attended a WLG (Women’s Leadership Group) Q&A with Eric Bocan, an Executive Sponsor for the Westlake location’s WLG. He discussed mobility, mentorship, and returning to the office. This wasn’t an event targeted toward interns so it was really interesting to be able to hear his advice for full-time employees and gave me some things to think about for longer-term career goals.
                </p>
                <p>
                  Tomorrow, I will continue working on my code. Specifically, I will split the LinearSVC and RandomForestClassifier algorithms into separate documents so I can evaluate their speeds separately. I will also look more into writing my predictions into an output document. The problem I am running into is that the current method for making predictions doesn't preserve the ids of the objects, so it is hard to map the prediction to the correct row and id in the spreadsheet.
                </p>
            </div>
          </div>

          <div id= "June-23-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 23, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Tuesday</p>
                </div>
                <div class="post-title">
                    <p>Things Are Starting To Make Sense</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I spent most of my day today in and out of meetings. This was kind of a nice change from last week, where I spent most of my time doing individual research. I didn’t make too much progress with my preliminary version of the NLP classification tool, but I got to know what the APT team was working on in a lot more depth, so I still had a full day of learning. I started my morning again with the Scrum call, then almost immediately after had a meeting about an update program. I wasn’t too sure what was going on in these calls since there were a lot of unfamiliar terms and acronyms being thrown around. Since I was confused, I reached out to Poornima to ask if she had time in her schedule for me to ask her some questions about what she was working on, clarify some terms, and give an overview of the workflow.
                </p>
                <p>
                  Rajiv and I also had our weekly 1:1 this morning. He explained more about the inner workings of how Fidelity uses databases and applications of how databases. I think this explanation put into perspective some of the tools that are being discussed in APT meetings. Rajiv even showed some real-time code examples, which was really nice especially since I don’t have experience with databases or SQL. I was able to demo what I worked on yesterday with my 75% accurate tool that compares a few NLP algorithms on the test data set of 200 rows. Rajiv suggested that I show Kiran what I have accomplished so far so I can get some feedback and ask questions, so he set up a meeting for the afternoon.
                </p>
                <p>
                  I had my meeting with Poornima this afternoon. She gave a very comprehensive overview of the Jira workflow and the APT production calendar. She also explained what happens at each stage of the project from start to finish (To-Do, Design, Development, User Acceptance Testing, Pending Migration, Certified), which will help me follow along with understanding the scrum calls. I also asked some questions about some terms and acronyms that I heard but did not understand. A lot of these terms have to do with SQL databases, so I will look more into the terminology on my own time as they come up. Poornima answered a lot of my existing questions about terminology though. I was especially confused about what dependencies, packages, modules, decommissioning, retiring, and schemas were in the context of the team, but it’s much clearer now! Poornima mentioned that when she first joined Fidelity, these terms were also very new to her and how it was almost like picking up a new language. This made me feel a lot better about being lost and asking so many questions and encouraged me that I might be able to understand the meetings by the end of the summer.
                </p>
                <p>
                  My last meeting today was demo-ing my tool for Kiran and Rajiv. Kiran and Rajiv seem happy with the progress I have made so far and it was nice to get some feedback after working on it mostly alone for the last few days. Some of the feedback mentioned from the meeting was that I might need to look into using a Fidelity local server. Currently I am running my code in Jupyter notebooks as part of the Anaconda suite, but it would be a lot more secure and a lot easier to implement company-wise if I was able to set up a local server. Since I don’t have prior experience with local servers, Rajiv suggested that I reach out to Himanshu, since he implemented a local server for his NLP tool. Further down the line, I would also like to ask Himanshu if he has time to take a look at my tool and give me some feedback based on his NLP expertise, since I am new to NLP and may not be doing things in the most efficient way. Lastly, Kiran suggested that I put my predictions into a formatted spreadsheet before showing it to the business side so that it is more of a finished prototype and we can really see how the tool would look in action. These suggestions are all things I plan to get started on tomorrow.
                </p>
                <p>
                  In between meetings I made some small tweaks to my tool in an effort to improve accuracy. I tried adding additional spreadsheet information so that I am training my tool on information from 5 different columns instead of just 3. This improved the accuracy from about 75% to 80%, which was exciting! I also tried running my data on a 2,000 row data set instead of the 200 row data set that I was initially using. This brought the accuracy up to 88%! :) However, I noticed that when I made the jump from 2000 rows from 200 rows, my program was noticeably slower. Efficiency is definitely something I will have to work on improving before I try to run this model on the large amount of data used in Spend (~200,000 rows). These initial improvements in accuracy is very exciting, but I anticipate that fine-tuning the model is going to take a lot more time and research than just adding information. I anticipate having to look into the algorithm’s inner workings and parameters to do this fine-tuning.
                </p>
                <p>
                  Tomorrow I don’t have many meetings, so I can work more on my code. I still have the warnings about ‘minimum split size’ that I was working with and it seems like this is coming from the fact that my data is relatively imbalanced and some categories have very few data members. I’m not yet sure how to address this issue without just feeding my model more robust training data. I’ll also try to wake up earlier so I can reach Himanshu and ask for his advice on local servers. I’m eager to see if I can understand more of tomorrow’s scrum call now that Poornima and Rajiv have filled me in on some of the high-level concepts and terminology.
                </p>
            </div>
          </div>

          <div id= "June-22-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 22, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Monday</p>
                </div>
                <div class="post-title">
                    <p>Somewhat Functioning First Attempt</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I started off my morning by joining the Scrum call again. I think I’m getting a better idea of what is happening now that I have been attending calls for about a week. Honestly, I’m still pretty lost, especially since my project is a bit disconnected from the database work that they are discussing. I think that if tomorrow’s scrum call is still just as confusing, I will reach out to Poornima and meet with her to ask questions about what the team is working on and what she is working on. Hopefully, this will give me more insight into what the scrum calls are about so that I can follow the calls more closely.
                </p>
                <p>
                  For most of the morning, I was trying to download nltk packages. The nltk library is quite large, so it took over an hour to completely download once I passed the right authentifications. While I was waiting during this time, I watched some videos about neural networks. I understand a bit more about hidden layers and activation functions. The downloads finished before I completed the videos, but I have bookmarked them so that I can watch them if I am waiting around or if I need a break from writing code. :) The Fidelity mentor chat was also pretty active this morning, so I talked to Jenna, Benjamin, Anushka, and some of the people Nancy added on Friday. Kaitlin added her mentee, Chelsea, so it was nice to meet someone new. Someone from the CTG interns made a group chat, so I’m hoping to get to know some interns within my business unit beyond just the introductions we had on Tuesday.
                </p>
                <p>
                  Once the nltk packages finished downloading, I tried getting to work using the libraries. I found a tutorial for using SVM and Naive Bayes on text classification instead of on numbers, which fixed the problem that I was having on Friday with using strings instead of floats. However, this tutorial was not accurate at all when applied to my data. SVM was only about 3% accurate and NB was about 3.5% accurate. I messed around with some parameters to try and see if I could get it working more accurately, but did not have much success.
                </p>
                <p>
                  I took a break from coding and talked to Nancy for a little bit about internships, college, and studying abroad. Since I noticed that a lot of people on my team are male, I wanted to ask Nancy about her experience in tech at Fidelity with the gender ratio. She mentioned that while there are more men than women overall, the ratio depends a lot on which team you are on. She mentioned that she was on a team where all the leadership was women and she mentioned that her LEAP class was pretty evenly distributed between men and women. This was reassuring to me and I think I just happen to be in groups that are more male-dominated. For example, our CTG intern group has about 20 people, but just 5-6 out of the 20 are women. The people that I have been in contact with on my team are also primarily male, with the exception of Janice and Poornima. WISTEM also has an event next week, but unfortunately this conflicts with a meeting. I’m looking forward to talking to Nancy more in the coming weeks and getting her advice on the internship and beyond. :)
                </p>
                <p>
                  Later in the afternoon, I found a different tutorial for Text classification, which used SVM, NB, RF, and logistic regression. This method took a while for me to implement since my data has more moving parts than the sample data. It ended up being worth it, since it’s much more accurate than the tutorial I was working with this morning! Each method is about 60-75% accurate. Of course, I will need to work on getting the model to be much more accurate before the Business group can use it since even 75% accuracy isn’t exactly reliable, but this is a good starting point compared to 3%! This tutorial has a lot more ways to visualize the data, which made the models seem much less ‘black box’ and made it a lot easier for me to see what was going on behind the code. The data set that David sent me last week has about 200,000 rows, but I am starting with 200 rows so that it's more manageable as I work through initial bugs and formatting, but the size of the data is something I will keep in mind as I move forward. There are still some changes I need to make to the tutorial’s parameters in order to fit my data. This is because the data I am working with has a lot of different categories and sometimes categories in the test data are not present in the training data, which means the model doesn’t know what to do with them. The large number of categories also makes the data visualization tools a bit difficult to read, so I may spend some time seeing if I can improve readability. There are some other warnings that I need to work through regarding ‘minimum split size’, but I’m not sure exactly what this means, so this is an area that I need to research more tomorrow as well.
                </p>
                <p>
                  Tomorrow I have my 1:1 with Rajiv, so I plan to update him on what I have been able to accomplish up to this point and what I hope to do next. I will also reach out to Poornima tomorrow so I have a better idea of what’s going on during the scrum meetings and so I can finally meet her face-to-face. I should have reached out to her sooner, but all the new information last week was a bit overwhelming so it totally slipped my mind. In terms of the text classification tool, I plan to address the warnings and research (and maybe even implement?) ways that I can improve accuracy.
                </p>
            </div>
          </div>

          <div id= "June-19-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 19, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Friday</p>
                </div>
                <div class="post-title">
                    <p>Finally Coding</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  Today I finished up my preliminary stage of research. This morning I got in contact with Himanshu and expressed my concerns about SVM working best only with small data sets. He said that SVM should be able to work with the size of the dataset that I am working with, but since I mentioned that I was looking into other algorithms, he recommended that I double check on the accuracy of those algorithms before implementing. Based on accuracy, I think that Random Forest would be preferable to Naive Bayes. However, since he mentioned that SVM still seems feasible, I will try SVM first. If I run into issues with SVM and the large data set, I will keep Random Forest (RF) in mind as a backup option. Since I had a plan for which algorithms I wanted to use, I tried to start coding at the end of the morning, but got stuck since I kept running into HTTP errors when installing Python packages through Anaconda.
                </p>
                <p>
                  In the afternoon, I had a bit of a break from coding and research. I attended the Fidtern meetup just like last week. It still felt a bit stilted since we mostly just introduced ourselves and talked about work, but it was good to see some familiar faces. I also had a short conversation with Korey from University Talent about my internship experience, especially since the internship is virtual this year. Around this time Nancy (my FYPN mentor) introduced us to some other software engineers (Gabriel, Hannah, and Kaitlin) as additional resources and connections. I really enjoyed connecting with more young full-time employees in the setting of a group, since I find it much easier to keep conversation flowing in a group with less pressure put on each individual person. Nancy set up a fun escape room activity with Anushka, Jenna, Benjamin, Kaitlin, herself, and I. We solved puzzles together virtually and I thought it was a cool interactive way to bond. :) At the end of the call, when we were talking about ‘highs and lows’ of our week, I mentioned that I was having trouble installing packages. Nancy connected me with a Senior software developer, Josh, on her team to help me out. Within about 10 minutes he was able to help me fix my issue and request elevated access so I wouldn’t have to worry about these problems in the future! It was really nice to get such personal and immediate help from a senior developer. I think this experience has encouraged me to reach out earlier when I run into issues. I was struggling for over an hour with this issue, but Josh and Nancy were able to help me fix it within 10 minutes!
                </p>
                <p>
                  With this problem fixed, I got into writing code in Jupyter notebooks. I learned how to read data from Excel to Python and how to explore some features of the data using built-in functions. I then tried to run this data through the SVM model. It took me a while to understand what the different pieces were doing, but I learned a lot through the process. However, I got stuck, since the data I am working with is not numerical. My data is categorized based on strings, whereas all of the SVM examples that I have found online are using floats, so I will have to find some workaround. I tried using RF as well, since I thought that RF would be less reliant on floats and it might be easier to make alterations to get it to work with my strings, but so far I have not had much success.
                </p>
                <p>
                  Poornima also reached out to me to check in and see how my first week was going. Rajiv has paired me with Poornima if I have questions and Rajiv is not available since Poornima is relatively new to Fidelity and so I might be able to relate to her experiences more. This week I have not had too many issues, since I have been mostly independently learning about NLP and ML concepts. I think that I get further into coding, I will probably be reaching out to her more as I run into questions.
                </p>
                <p>
                  Next week, I hope to continue working on getting either the SVM or RF model (or both?)  working on my small test set of data. If I can get this working, then I can see what happens to the accuracy as I apply it on a larger scale and make some alterations from there. This first week has been pretty fun and I’ve learned a lot of new information. I’m looking forward to taking the weekend to process the new information and experiences and hopefully next week I’ll be doing more coding and continue making connections!
                </p>
            </div>
          </div>

          <div id= "June-18-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 18, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Thursday</p>
                </div>
                <div class="post-title">
                    <p>Exploring Algorithms</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  Today was a bit of a roller coaster when it comes to where my research has taken me. I spent my morning learning about the Support Vector Machines (SVM) algorithm that Himanshu mentioned yesterday. A brief summary of what I learned is that SVMs plot data into an N-dimensional space, then the algorithm creates a hyperplane that categorizes the data appropriately. This is a very popular ML supervised algorithm and is preferred to a lot of other classification models because of the kernel function, which can map the data to higher dimensions in order to better fit the data if the initial dimensions don’t allow for the data to be categorized well enough by the existing Support Vector Classifiers. I followed a toy example for using SVMs to classify if a recipe was for cupcakes, muffins, or scones based on the percentage of butter and sugar in the recipe. (Apparently cupcakes have pretty high amounts of both butter and sugar, muffins have low amounts of both, and scones have a lot of butter but not a lot of sugar). 
                </p>
                <p>
                  The SVM algorithm seemed to have a lot of promising qualities, especially since it was not too computationally or memory expensive and had high accuracy. However, upon reading more detailed documentation and articles,  quite a few sources mentioned that it works best with smaller data sets. From what I understand, the data I am working on does not seem to be a small data set, so I'm not sure if SVM is the best algorithm. I looked into other algorithms used for classification. The two that seem most promising are Naive Bayes and Random Forest. (Other algorithms like Tree Decision and K-Nearest Neighbors don’t seem right for this project based on incompatibility with accuracy or categorical data).
                </p>
                <p>
                  Naive Bayes has the pros of requiring less computational power, accurately working on large datasets, being fast, being pretty easy to implement (supposedly), and being best suited for text classification so it could be used for the sentiment analysis application that David mentioned last time. However, some tradeoffs are that it makes the strong assumption about the features to be independent and implements zero frequency, which means if the category of any categorical variable is not seen in training data set then model assigns a zero probability to that category and then a prediction cannot be made, so might not be able to fulfill the 'flexible and dynamic' part. Random Forest has the pros of having high accuracy, flexibility, and less variance. Also it’s supposedly not too difficult to implement, works well in handling missing values and detecting outliers, and can identify the most important feature among available features (not sure how relevant this is for the current project). However, it has high computational and memory cost. Additionally, as the number of trees increases, the algorithm can be slow. So, it's not necessarily a problem if the data set itself is large (i.e. a lot of rows in the spreadsheet), but it could be a problem if there are a lot of categories to classify the data into. I’ll ask about the order of magnitude of categories tomorrow!
                </p>
                <p>
                  I spent a little bit more time looking at Deep Learning concepts, but I think my priority is learning these categorization algorithms for now. Tomorrow, I plan to ‘get to work’ earlier in the morning so I can ask Himanshu what his opinion is for SMV if my data set is of a larger scale or if he has additional suggestions for algorithms that I should look into.
                </p>
                <p>
                  This morning, I also had a short meeting with Janice who is the Vice President of IT Management. We talked about my goals for this internship and she gave me a lot of advice and insight about Fidelity as a place to consider working full time. She emphasized how Fidelity cares for its employees through benefits, a flexible environment, and lots of opportunities for mobility within the company. She suggested some ways to get more involved with the culture as well. This has definitely given me a lot to think about when considering if I’ll consider Fidelity as a future employer! :)
                </p>
                <p>
                  I sent an email to David, Cameron, Daniel, Anuj, Kiran, and Rajiv about the current rule-based system for classifying data, so hopefully I’ll get a response tomorrow to clarify those questions and help me find the best suited algorithm for this project! I also have a check-in with Korey from University Talent to talk about how the internship is going so far, so it’ll be nice to catch up with her. I plan to spend tomorrow looking more into Random Forest and Naive Bayes from some different sources. I keep seeing Principal Component Analysis (PCA) mentioned as an algorithm for data analysis and predictions, so I plan to look into if that’s another feasible algorithm tomorrow. Another full day of research and learning new things ahead!
                </p>
            </div>
          </div>

          <div id= "June-17-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 17, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Wednesday</p>
                </div>
                <div class="post-title">
                    <p>Old and New Directions</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I started my morning off by joining the Scrum Meeting as Rajiv suggested yesterday. After what Rajiv explained to me on Monday, I understand a bit more of what is happening at a conceptual level with the way that the Agile method works and Sprints and the daily updates. However, I am still quite lost when it comes to the actual tools and modules being discussed. I’m sure that as time goes on and I attend more of these meetings it will become less confusing. For now, I will try and keep attending these meetings to become more familiar with the workflow within Fidelity.
                </p>
                <p>
                  Later in the morning, I met with Himanshu and Rajiv to talk about my project. Himanshu is part of the team in India, and he has worked with NLP in the past, so he was able to offer a lot of guidance and helpful information when it came to pointing me in a more focused direction for my research. In particular, he suggested that I check out the Support Vector Machines (SVM) library, since this library is often used for categorization. He also advised that I will probably need to use custom logic for the machine to be able to process Fidelity specific data since pre-existing data sets like NLTK might not be specific enough to cover the vocabulary used in Spend. We talked a little about unsupervised and supervised learning, since it seems that the Business team wants a model that uses unsupervised learning for their flexible and dynamic model. Since Himanshu is in India, my work hours are late into the night for him. To communicate with him in real time, I’ll wake up about an hour earlier, otherwise he suggested that I leave messages for him at the end of my work day and he will respond to them in his morning hours. I’m looking forward to hearing more about the insights he gained from his NLP project in the future and applying it to my own project!
                </p>
                <p>
                  I finished the introductory neural network course that I started yesterday during the rest of the morning and into the early afternoon. I think this course is a good starting point, but if I were to implement deep learning, I anticipate that I would need to find more resources about how to utilize the neural networks and other deep learning concepts that were mentioned in the tutorial.
                </p>
                <p>
                  After lunch, there was a Fidtern workshop about personal branding. I thought this workshop was pretty interesting. Although the workshop covered a lot of material that I had already learned from Mudd’s career workshops, it was interesting to hear it from another viewpoint. I noticed that there was an emphasis on soft skills during this workshop, which is something I can work on displaying more in interviews and on my resume. Around this time I also had some conversations about TV and movies with my fellow interns from FYPN. It seems that as we all get busier, people have less time to talk, but I hope that we can still keep up these conversations in whatever free time we find!
                </p>
                <p>
                  For the remainder of the work day, I started the Stanford NLP and Deep Learning course and started looking into the SVM library that Himanshu recommended. While the Stanford course is very informative, I think that it goes more into the math behind the tools than I am looking for. As ChunLei mentioned earlier this week, my focus for this project shouldn’t be on the algorithms portion of Machine Learning, but rather how these ML algorithms are applied. Of course, to apply the algorithms, I should understand how they work to some extent, but since the Stanford course seems to focus more on the inner workings of the algorithms than the applications, I don’t think I will be continuing with the Stanford course since it doesn;t align with my goals. I also started looking into the documentation for SVM, and I think I understand the basic ideas. Tomorrow I will dive deeper into fully understanding the library. SVM seems much more suited to my project than the NER library that I found earlier this week since it focuses more on categorization based on word semantics than categorization based on sentence structure. :)
                </p>
                <p>
                  I also started installing the necessary tools for starting to write some code, but didn’t have time to finish setting it up, so I will continue to do this tomorrow. I’m not familiar with the Fidelity systems and what restrictions there are on downloads, so I’m trying to take this part slowly and carefully to avoid running into trouble later. Tomorrow, in addition to more research and installations, I also have my meeting with Janice to look forward to.
                </p>
            </div>
          </div>

          <div id= "June-16-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 16, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Tuesday</p>
                </div>
                <div class="post-title">
                    <p>Networking: Neural and Otherwise</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  Today I spent more time collecting resources and learning about NLP. In particular, today I started looking at resources about Neural Networks. I found an introductory Neural Networks tutorial on Youtube, which requires no prior knowledge. I started working through this course and plan to use this as my jumping-off point before diving into more advanced resources. I found other courses and articles about Neural Networks and NLP. In particular, I found entire courses from Stanford, one of which is about NLP specifically and another which is specifically about NLP with Deep Learning. I think that once I finish the introductory neural network course I will try the Stanford NLP with Deep Learning course, and if it is still too advanced to follow and I will go back to more basic courses. I also found a few more resources about NER. In particular, I started looking into the documentation for spaCy. I only have a basic overview, so I will go back later and read more of the details when it comes time to start building the tool.
                </p>
                <p>
                  Today I also looked into some possible packages that I might use in the process of pre-processing the text,, such as BeautifulSoup and NLTK. I looked at the capabilities of these tools and how I might use them in practice. In the coming days, I will look into more packages for feature extraction and classification, since I anticipate that this will be the bulk of the work when it comes to being able to accurately classify the Spend data. In the past two days there has definitely been an overwhelming amount of new information, but writing it down here and trying to absorb it little by little has definitely been helping. :)
                </p>
                <p>
                  Today I also got to contact a few different groups of people. So while I am doing quite a bit of individual research, I am still able to reach out to Rajiv and other team members as well as other interns.
                </p>
                <p>
                  This morning Rajiv and I had our first weekly 1:1. During this time, I got to talk about my personal career aspirations and what I was hoping to gain from the internship. I talked about my experience working with front-end and back-end and how I was interested in ML and AI, but there didn’t seem to be many opportunities in ML for undergraduates. Rajiv offered some helpful insight about how fast the CS industry moves and how in 2 years when I graduate, ML might be much more prevalent and how job opportunities could very well open up for undergraduates like me. Rajiv also taught me about Cloud products, specifically SAS, and how they are integrated into Fidelity as well as our everyday lives. I enjoy learning about current relevant technologies used by Fidelity during these meetings with Rajiv. He also suggested that I come to more Scrum meetings so that I can learn more about the corporate workflow, specifically with Agile. I will definitely take him up on that offer and I plan to attend tomorrow’s Scrum meeting!
                </p>
                <p>
                  This afternoon, we had a meet-up with CTG leadership and interns. The meeting was pretty casual, and we went around introducing ourselves and having some conversations about the COVID-19 situation. It was nice to be able to meet the other interns that I might have been working alongside or even sharing office space with in Westlake. I connected with some interns on LinkedIn after the meeting and one intern reached out to me and we had a brief conversation about our projects. It’s cool to hear about other interns’ projects and have the “water cooler conversations” that we might have had back in the office. I also kept in touch with my mentor group throughout the day.
                </p>
                <p>
                  A couple of questions came up during my research from yesterday and this morning, so I messaged all the members of yesterday’s meeting. David responded very promptly and answered a lot of my questions and even gave me some sample data to look at! At first glance, the data is pretty overwhelming. I don’t fully understand it yet, but I will take a better look at it once I get more involved with the specifics of the project. Rajiv also informed me that Janice invited me to Thursday’s FPAIT staff meeting, where I will introduce myself. I have yet to receive the email invite, but I am looking forward to meeting even more people!
                </p>
                <p>
                  Tomorrow I plan to continue my introductory neural network course and hopefully start the Stanford course. Additionally, Rajiv has set up a meeting for me to introduce myself to a member of the team from India after the morning Scrum meeting, which I am excited about. There is also a Fidtern workshop about professional branding that I hope to attend in the afternoon. Looking forward to another busy day of learning!
                </p>
            </div>
          </div>

          <div id= "June-15-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 15, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Monday</p>
                </div>
                <div class="post-title">
                    <p>Focusing My Research</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I spent most of my time today independently researching the more basic concepts of NLP. and the general workflow of data science. I found a particularly helpful video was this video from PyOhio about NLP in Python. The speaker was Alice Zhao and I found that the way she explained the concepts was very clear and approachable for someone like me who is new to NLP and Data Analytics as a whole. I spent most of my morning watching this lecture and taking some notes to come back to later. From this video I gained a basic understanding of the process of NLP and the kinds of insights that can be drawn, but I also found that there were some topics that I was interested in looking further into. For example, I would like to look more into using Pandas, python packages, and stemming.
                </p>
                <p>
                  In the early afternoon, I also looked into other resources, such as the ones that ChunLei sent me on Friday. I looked into resources like Vik’s Blog, towards data science, and sentdex. These resources brought up some very interesting concepts that I had not seen before, and which I will definitely need to look more into. From Vik’s Blog, I think I will research more about Linear Regression, since this is a concept that has come up in my Scientific Computing course, but I would need to do more research to see how exactly it is being applied to NLP and this project. From toward data science, I am interested in doing more research about what tool might be best suited for this project. I know that Python was mentioned before, but it may also be worth looking into R and TensorFlow. I also learned about the difference between the classical ML model and Deep Learning. Classical ML is where you train the model using feature extraction on the input and then classification on the extracted features. Deep Learning is where the feature extraction and classification happen together in one step using neural networks. From sentdex, I will definitely look more into NLTK, since it seems to have some extremely useful built-in tools. 
                </p>
                <p>
                  ChunLei directed me to a site called Amazon SageMaker. He recommended that I not worry too much about the math and the details of the project, but rather get a better understanding of high level concepts. After reading through the documentation, I found that one particular feature that might be useful for this project is Named Entity Recognition or NER. This feature sifts through text data to locate phrases and categorize them into labels like person, organization, or brand. I think that this labelling could be extremely useful for Spend Categorization. However, one problem is that based on the resources that I have seen so far, it seems that NER is used in sentences, whereas I am not sure what form our data will look like. Another problem is that NER categorizes sequences based on predefined labels, which would not be cohesive with the desire for a more dynamic model. NER is definitely a topic that I will spend time researching more.
                </p>
                <p>
                  Later, I had a meeting with Rahiv, Kiran, Cameron, and some new faces including Daniel and David from Business, and Anuj from Kiran’s team. We talked more about the expectations of this project and the pacing. David offered some insight about stemming vs. lemmatization, and suggested that lemmatization might be more accurate. I agree with him and would definitely implement this approach when it comes to that point in the project. I asked some questions about if previous projects have used classical ML or deep learning, but it seemed that the decision of which is best suited for this project will be left up to me. We ran out of time during this meeting since there was a lot to discuss, but I am looking forward to sharing more about what I have learned and asking more questions in the future.
                </p>
                <p>
                  At the end of the workday, I started looking more into classical ML vs Deep Learning. It seems like classical ML is much more approachable and may be easier to implement, but Deep Learning will probably do a better job of accomplishing the dynamic and self-learning model that Business is asking for. I need to do more research about Deep Learning and specifically Neural Networks to see if this is something that I would realistically be able to learn and get started on during the summer. The best and most efficient ways are always the hardest to implement :,). 
                </p>
                <p>
                  Unrelated to work, I found out that I have some connections to other Fidterns! An intern in my mentor group, Benjamin, knows a friend from high school, and another SWE Fidtern, Cassie, knows one of my friends from Mudd! Additionally, I’ve continued talking to my mentor group, and today Nancy added a new member, Jenna, who shared pictures of her adorable dog. :)
                </p>
                <p>
                  Tomorrow, I plan to start looking into some of the topics that I came across today, particularly classical ML vs Deep Learning, Neural Networks, NER, Automatic Feature Extraction, and word2vec. I don’t anticipate that I will be able to get to all of these topics tomorrow, but it’s a starting point into more research! Tomorrow there is also a CTG intern meet-up in the afternoon, where I’m looking forward to meeting other interns in my business unit. 
                </p>
            </div>
          </div>

          <div id= "June-12-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 12, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Friday</p>
                </div>
                <div class="post-title">
                    <p>The More I Learn, The More Questions I Have</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  Today was the last day of webinars! It was nice to learn so much during training, but honestly, I think I’m ready for a change of pace. Sitting through long Zoom meetings is getting quite exhausting and I’m looking forward to learning more technical skills through projects. This morning we had a pretty information-heavy webinar about HR and specifically about the protocol for logging the hours we work. Then in the early afternoon we played some fun icebreaker games. At noon we had an intern meet-and-greet session where I got to know some interns from Massachusetts and New Hampshire.
                </p>
                <p>
                  I had a meeting with Kiran today to update him on what Cameron and I talked about yesterday. Kiran was able to provide me with a lot more detail about the project’s current status and what my role on the project might be. He also put me in contact with an NLP specialist, ChunLei. ChunLei was able to explain a lot of insight about NLP and how it would play into what the Business team was asking for. ChunLei also pointed me to some really great introductory resources that I was able to get started with.
                </p>
                <p>
                  For the rest of the afternoon, I looked into the resources ChunLei recommended as well as some resources that I found on my own during my online research. I have learned a lot of introductory material about topics like NLP, Deep Learning, Tokenizing, etc. In the coming week or so, I anticipate that I will be spending a lot of time self-studying these topics and getting more familiar with NLP at a high conceptual level. I have found that the more I learn about these topics, the more questions I have for the Business team regarding what they expect for their new Spend tool. For example, I am wondering what kind of data I will specifically be working with and how I might access the data if it is from 3rd party sources. I am also wondering about the new categorization aspect. What would necessitate new categorization? How specific would the categorizations be? How generally can we lump items together? Lastly, I would wonder if there is a strong preference for Python because of previous infrastructure or ease of implementation into the existing tools or if this preference is due to the existing packages? I will definitely try and ask these questions in Monday’s meeting with the Business team and with Kiran.
                </p>
                <p>
                  Throughout the day, I also remained in contact with my mentor group, which was a lot of fun! Next week, I hope to keep in contact with the interns I have met this week and to keep learning about new topics like NLP. I am still getting used to the corporate structure and making sure that I am on top of my meetings.
                </p>
            </div>
          </div>

          <div id= "June-11-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 11, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Thursday</p>
                </div>
                <div class="post-title">
                    <p>A Potential Project!</p>
                </div>
            </div>
            <div class="post-body">
              <p>
                This morning, like the 3 mornings before, consisted of more webinars. Today we learned about strategies we can use to make the most of our internships, like making connections and getting involved in different groups. Later, we heard from FYPN (Fidelity Young Professionals Network) and some details about the mentorship program. Coincidentally, my mentor reached out to me today and I got to meet her for the first time (more on that later).
              </p>
              <p>
                In the afternoon, I had my meeting with Cameron from Procurement Business Analytics. Unfortunately, Daniel had a last-minute conflict and could not make it to the meeting. I got to introduce myself to Cameron and hear about the project that I might be able to get involved with. From his explanation, I think I would be interested in joining this project. I’ll try and explain the main ideas of what I learned, but there was quite a lot of new information to absorb. Procurement is a department that deals with purchasing items for Fidelity, which can include anything from software and hardware equipment to the desks and chairs used in office. Procurement has to classify these purchases into the appropriate categories to log their spending. The current model is using a rule-based system. However this model is not very flexible, so the project I have the opportunity to get involved with would classify the spending using NLP (Natural Language Processing) using past data of similar spend and possibly enrich the data with third parties to align with industry changes. I’m not totally sure what this means quite yet and some of the terminology is confusing, but I will definitely do some research so that I can come up with more specific questions to ask at the next meeting! I haven’t worked with NLP before this, so I will definitely have a lot of new things to learn and catch up with.
              </p>
              <p>
                Cameron mentioned that this project is in its very early stages of planning, so on the bright side, this means that I might not have as much catching up to do in regard to this project before I get involved. I’m not quite sure who exactly I would be working with, but since I am new to NLP, I would love to have some guidance or specific people I can reach out to with NLP questions. I think this would make the learning process a lot more comfortable and allow me to make more connections. :) Tomorrow I will meet with Kiran and Rajiv to update them on what I talked to Cameron about.
              </p>
              <p>
                This afternoon, I also met with my FYPN mentor, Nancy! We talked for about half an hour mostly about non-work related things, which was a nice break. I was added into a group chat with two other interns who are Nancy’s mentees, Anushka and Benjamin. It was nice to be able to connect with other interns and hear about the experiences of someone newer to Fidelity. I look forward to talking to them more throughout the next couple weeks.
              </p>
              <p>
                Tomorrow is the last day of training! I’m looking forward to getting started on projects and learning as well as connecting with other interns.
              </p>
            </div>
          </div>

          <div id= "June-10-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 10, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Wednesday</p>
                </div>
                <div class="post-title">
                    <p>Diversifying Connections</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I started off my morning by joining the APT Scrum meeting. I got to meet the Accounting Dev Team as well as some of the India team. As mentioned yesterday, the Scrum call was a daily status call where the team updates each other on the progress they are making and brings up any problems that they are facing. Specifically, I got to hear from Ramya, the Scrum Master for APT. I got a rough idea of the workflow within the team, but to be completely honest, I had no idea what was going on in terms of the projects that were being discussed. It was a bit intimidating to have so many new concepts swirling around, but on the bright side, this means that I’ll be able to learn a lot of new things!
                </p>
                <p>
                  After the Scrum meeting was the start of training for the day. The majority of training today was about ERGs, which are Employee Resource Groups. We got to hear from the heads of many groups such as Fidelity Pride, AERG (Asian Employee Resource Group), FiVe (Fidelity Veteran Employees), WLG (Women’s Leadership Group), Fidelity Enable, and WITSIG (Women in Technology Special Interest Group). This webinar was quite long, but it was a good way to hear the insights from Fidelity members across many different business groups. After the session I signed up to join AERG, WLG, and WITSIG. I’m looking forward to getting involved and meeting new people from different fields via these groups. :)
                </p>
                <p>
                  Later we had a shorter webinar from Fidelity Cares, which told us about how Fidelity is involved in local communities. As part of the webinar, we participated in a project where we recorded ourselves reading children's books to be sent to local Boys and Girls Clubs across the country.
                </p>
                <p>
                  In the afternoon, I met with Rajiv to talk about his experience at Fidelity as well as to learn more about what happened in the Scrum meeting from the morning. I got to hear about Rajiv’s journey from being an international student studying electrical engineering to getting involved in databases via Oracle and finally working at Fidelity. I also learned a lot about the Waterfall and Agile methodologies. From what I understand, the Waterfall method includes a lot of planning and iterative testing before developments are rolled out. On the other hand, the Agile method includes more frequent deployments based on input from the user and follows the ideology of “pacing over perfection.” Rajiv explained to me that APT uses Agile, with Scrum meetings as a way to check up on the individual members of the team and progress they are making or the problems they are facing. This definitely cleared up a lot of what I was wondering about the workflow and team dynamics at a high/conceptual level, but there is plenty more for me to learn about the process and dynamics of the team. Perhaps I will read up on Agile and Scrum in my free time this week!
                </p>
                <p>
                  Lastly, Rajiv asked me to set up a meeting with Cameron from Procurement Business Analytics. I sent a Zoom invitation to him for 3pm EST tomorrow, where I will be meeting with him as well as Daniel. This meeting is just for me to introduce myself, since it seems like I may be working with them during my internship, as well as get an idea of what their team is working on.
                </p>
                <p>
                  Looking forward to more training and meeting new people tomorrow!
                </p>
            </div>
          </div>

          <div id= "June-09-2020-FI">            
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 9, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Tuesday</p>
                </div>
                <div class="post-title">
                    <p>Getting Used To It All</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  Today was pretty similar to yesterday in terms of having mostly webinar training and getting to know the team that I will be working with this summer a bit better. I think I am getting more comfortable with the platforms that Fidelity uses to communicate across the firm, but there is still quite a lot to get used to over the next week. I signed up to be on Digital Champions on Yammer and posted an introduction, which I hope will help me connect me with other interns this summer.
                </p>
                <p>
                  My morning started with a few webinars. We had a brief overview of Fidelity’s history, structure, and financial services. I thought this was a good way to contextualize the work that we will be doing in the upcoming weeks. Some of the terms mentioned were familiar to me from the Econ courses I’ve taken, but there was plenty of information that was very new to me! The next two webinars were about the technology tools that Fidelity uses. We learned about the different roles of Yammer, Microsoft Teams, Skype, Fidelity Central, Workday, Outlook, and Zoom. While this helped me understand the tools, I am still a bit overwhelmed with the amount of locations to check information on. I think I will become more familiar with this software as I continue to use it.
                </p>
                <p>
                  Later in the afternoon, I got to talk to my manager, Rajiv, and meet Kiran, who is in charge of procurement. I think that I will probably be working closely with Kiran’s team since they work primarily in Python, which is one of my strongest languages. It was nice to talk to Rajiv and Kiran and hear about their experiences at Fidelity. Based on what I’ve heard Fidelity seems to have a flexible and balanced work environment!. I will also be attending tomorrow morning’s daily scrum meeting, just to listen in and meet more of the team. :) I look forward to hearing about what my mentors/coworkers have been working on and getting to work alongside them and learn skills from them throughout the internship.
                </p>
                <p>
                  Tomorrow’s training will likely include more webinars and getting familiar with Fidelity’s technology and culture. I’ll try and keep active on Yammer and engage with the team I will be working with, even though we are virtual!
                </p>
            </div>
          </div>

            <div id= "June-08-2020-FI">
                <div class="post-header">
                    <div class="post-date">
                        <p><strong>JUNE 8, 2020</strong></p>
                    </div>
                    <div class="post-dow">
                        <p>Monday</p>
                    </div>
                    <div class="post-title">
                        <p>First Day As A Fidtern!</p>
                    </div>
                </div>
                <div class="post-body">
                    <p>
                      Today was my first day as an intern! This week will primarily be training via online webinars and some meetings with my manager, but I am excited to start learning more about Fidelity and the role that I can play within the firm.
                    </p>
                    <p>
                      I started the morning off by setting up my work laptop. I received two laptops and I am not sure if this was intentional and I will be using both or if this was a mistake. I have reached out to the University Talent team to ask about this issue. I then spent about 2 hours going through my Fidelity email’s inbox and getting acquainted with the different platforms that the firm uses, such as Yammer, Microsoft Teams, Zoom, etc. I found it interesting that Fidelity uses so many different platforms. I am still a bit overwhelmed with trying to keep up with all the platforms, but I anticipate that there is a high learning curve and I will get used to using all these platforms to connect with my new co-workers! :)
                    </p>
                    <p>
                      At 10am we moved onto the webinars. Today’s webinars included a welcome webinar which welcomed all interns to the firm, a Cyber Safety webinar which educated interns about how to protect ourselves and Fidelity from cyberattacks, and a Intentional Connections webinar which addressed how to create meaningful personal relationships while working remotely. I found these webinars pretty informative and I especially enjoyed the Intentional Connections webinar, since I find it a bit more difficult to communicate with people virtually instead of in-person. I am looking forward to more useful information via webinar throughout the week!
                    </p>
                    <p>
                      I was also able to get in contact with my manager (Rajiv) this morning. He introduced me to another member of the team and discussed what I might expect from the internship. It seems that I will be on the Oracle team. Rajiv mentioned that they work closely together with the procurement team, so my role may include being a liaison between the teams. He specifically mentioned that I might be doing some documentation and working with Python and Java during my internship. I am looking forward to learning more about Agile and development releases, since Rajiv mentioned these two areas which I don’t have much previous experience with. This seems very different from my CS experience last summer and I am looking forward to learning a lot of new CS applications through this new experience!
                    </p>
                    <p>
                      Tomorrow holds another full day of webinars and hopefully I can meet more of my fellow interns as well as getting to know the team I will be working with this summer!
                    </p>
                </div>
            </div>
        </div>

        <div class="proj-footer">
            <p>
                This summer I am working as a Software Engineer Intern as part of the CTG (Corporate Technology Group) business group. Specifically, I am working as part of the FPAIT (Finance Procurement Accounting Infrastructure Team).
                <br> I am working in close partnership with the Business Procurement Team to implement NLP into their Spend classification process so that it is more flexible and requires less manual upkeeping than the current rule-based model requires.
                <br> <em> More details to come as I settle into my internship role!</em>
            </p>
        </div>
    </div>

    <!-- JavaScript for Navigation Bar-->
    <script>
      function openNav() {
        document.getElementById("mySidebar").style.width = "250px";
        document.getElementById("main").style.marginLeft = "250px";
      }
      
      function closeNav() {
        document.getElementById("mySidebar").style.width = "0";
        document.getElementById("main").style.marginLeft= "0";
      }

      window.onscroll = function() {myFunction()};
      var header = document.getElementById("myHeader");
      var sticky = header.offsetTop;
      function myFunction() {
          if (window.pageYOffset > sticky) {
              header.classList.add("sticky");
          } else {
            header.classList.remove("sticky");
          }
      }
    </script>

    <!-- Optional JavaScript for Bootstrap -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS-->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>

  </body>
</html>