<!DOCTYPE html>
<html lang="en">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta charset="utf-8">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    <!-- Custom CSS tyle Sheet link -->
    <link href="../style.css" type="text/css" rel="stylesheet">

    <!-- Title -->
    <title>Vivian's Blog</title>
  </head>

  <body>
    <div id="mySidebar" class="sidebar">
      <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">&times;</a>
      <a href="../index.html">Home</a></li>
      <a href="./gamified.html">Gamified Task Manager</a></li>
      <a href="./fidelity.html">Fidelity</a></li>
      <a href="./greencrew.html">Sustainable Claremont</a></li>
      <a href="../about.html">Info</a>
    </div>

    <div id="main">
      <button class="openbtn" onclick="openNav()">&#9776;</button>
      
      <!-- Project Header -->
      <div class="proj-header">
        <div class="proj-title">
            <h1>Fidelity Internship</h1>
        </div>
        <div class="proj-dates">
          <p>June 2020-Present</p>
      </div>

        <div class="proj-status">
            <p><em>STATUS: In Progress</em></p>
        </div>
      </div>

        <!-- shortcuts to top and bottom -->
        <div class="proj-shortcuts">
            <div class="proj-shortcuts-top">
                <p><a href="#June-30-2020-FI">&uArr;<br>To newest</a></p>
            </div>
            <div class="proj-shortcuts-bot">
                <p><a href="#June-08-2020-FI">To oldest<br>&dArr;</a></p>
            </div>
        </div>

        <!-- Blog posts -->
        <div class="posts">
          <div id= "June-30-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 30, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Tuesday</p>
                </div>
                <div class="post-title">
                    <p>Parameter Fine-Tuning</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I started my morning with 3 back-to-back meetings with the FPAIT APT team. We started with the usual daily scrum call, where I am  able to follow along but I’m still not familiar with the specifics. I think that unless I get directly involved with the database system it will be difficult for me to understand the ins and outs of the daily scrum calls, but it’s still nice to check and see what the team is up to. Next, there was an end-of-month meeting discussing what was accomplished in June. This gave a broad view of what the team had accomplished during the month and what still needed work in July. One part of the meeting that stuck out to me was at the end of the meeting when we had a couple minutes to “shout out a coworker” which was when team members could praise others for exceptional work. I thought this was a nice way to celebrate team members and to break up an otherwise formal meeting especially because the current remote work environment can make it hard to feel connected to other associates. Lastly, there was a meeting about the current update. After last week’s meeting with Poornima, I had a better understanding of what the update was about and what aspects were being discussed, but again I’m not too familiar with the specifics of the update components.
                </p>
                <p>
                  I spent the remainder of my morning and early afternoon exploring parameters for LinearSVC. Since I decided on LinearSVC as my algorithm of choice for today, I wanted to experiment with changing the default values of the parameters to see if I could improve the accuracy at all. I found that the parameters that maximize output vary depending on the size of the data set. I initially experimented with values for my 2k row data set, but when I applied the same parameters to the total 200k data set, it actually decreased the accuracy. Ultimately, I found that the variables that affect the accuracy of my data are C, fit_intercept, and intercept_scaling. Other variables like tolerance, verbose, and max_iter generally don’t affect the accuracy or the default value consistently produces the most accurate results. Ultimately, changing the parameters didn’t make a huge difference, since it just increased my accuracy from 95% to 96%, but on a larger scale this small percentage change could be a difference of hundreds or thousands of values being correctly categorized. I’m pretty happy with the progress I’ve made for now. :)
                </p>
                <p>
                  This afternoon there was a ‘Lunch and Learn’ panel with Personal Investing and how they’ve been responding to market volatility and Covid-19. Since my project and my team are not directly involved with the markets or investing, it was interesting to learn about a completely different field under the same company. I think I’ll continue to make an effort to attend a variety of ERG events since it seems like a good way to get to know what’s happening in other branches of Fidelity.
                </p>
                <p>
                  Today I also got involved with the Digital Champions program in the intern group, which is designed to help promote engagement, plan events, be a technological leader, and give real-time feedback. There are a couple options for how to get involved, and I think I can realistically commit to the 21-Day challenge which has daily 5-10 minute tasks and the Elite 21-Day challenge which has daily 10-20 minute tasks. The other more intense programs might not be realistic for my current workload, but if I find myself with free time I’ll definitely look into them! There were a couple of quick tasks today about navigating Microsoft Teams and Outlook and I’m excited to see what else is in store.
                </p>
                <p>
                  For the rest of the afternoon, I looked into different methods for vectorizing text. I am currently using TF-IDF (Term Frequency-Inverse Document Frequency) which pulls out the most ‘important’ words for each category in a document based on its frequency of appearance for a category compared to the entire document. I’ve come across other vectorization methods like word2vec and gloVe during my research phase in the first week and in the article that David sent. Ultimately, it seems like TF-IDF works best if you have a large labeled data set (which I do). Word2vec and gloVe are strongly favorable only if you have sparse training data, but when you have >10k rows of training data, TD-IDF outperforms other methods, so I will stick with TF-IDF. I also experimented with TD-IDF's parameters to see if I could further increase the accuracy, but it seems as though the values I have been using are optimal. For now I think I’m done experimenting with parameters to increase accuracy, but I’ll see if Rajiv has further suggestions for how to improve accuracy and/or efficiency tomorrow.
                </p>
                <p>
                  It seems like the end of the month is a busy time for a lot of people! Rajiv rescheduled our meeting for tomorrow morning, so I look forward to updating him on my findings and my progress then. I also have my meeting with Nancy tomorrow, where I’ll ask the questions that I compiled yesterday. Since I’m happy with my progress with my classification tool for now, I plan to spend my time tomorrow looking into servers. I don’t have prior experience setting up or using servers, so I’d like to learn more before my meeting with Don on Thursday so that the meeting is as productive as possible and I can follow along with any suggestions he has without needing to ask for clarification on the more basic terms.
                </p>
            </div>
          </div>

          <div id= "June-29-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 29, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Monday</p>
                </div>
                <div class="post-title">
                    <p>Deciding On LinearSVC</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  This morning, after the scrum meeting I fixed my method of random sampling. Last week, some of the data that I was writing prediction outputs for was duplicated from the training data, which meant that my results were misleadingly high. I fixed my method so that I removed some data randomly using the .sample() and .drop() functions at the beginning before the model was trained to save for testing and writing predictions. This allowed for more realistic results. For a set of 200 rows, the model was about 70% accurate and for a set of all 200k rows, the model was about 95% accurate. This matches the accuracy levels seen last week for Linear SVC when comparing algorithms. There is still quite some work to do for fine-tuning the model to be even more accurate.
                </p>
                <p>
                  There was also a panel with some of the executive leadership at Fidelity to answer questions from interns. We got to hear from Abby Johnson (President and CEO) and members of her senior leadership team on topics like current events, advice to their younger selves, professional growth, business strategies, etc. It seemed like the members of the senior leadership team were pretty close and made conversation and jokes throughout the webinar, which was a pleasant surprise since a lot of the webinars so far have been pretty strictly following a pre-set schedule. A lot of the leadership also mentioned their experience working at companies prior to joining Fidelity and I think that these were often the most insightful topics. A lot of the advice they gave had to do with taking risks and opportunities when possible and not ‘sweating the small stuff’ during college and the beginning of your career. This was nice to hear, especially coming from a group of people who have achieved such great things in their careers. Hearing about leadership roles from this panel definitely got me thinking about more managerial roles and job opportunities in my future.
                </p>
                <p>
                  Today I also got in touch with Don from the System Engineering team about setting up a Fidelity server. I gave him a brief overview of my project and the scalability concerns I was having and asked if he had any insights. However, since setting up a server is a pretty technical matter, he asked to set up a meeting with Rajiv and I to talk through some of the more technical aspects before offering advice or recommendations. It seems like both Rajiv and Don are pretty busy this week, so the first reasonable availability that I could find was for Thursday afternoon. I’m looking forward to working towards some of the next steps and looking into how I might be able to scale my tool. I’ve never worked with servers before, so this is another new field that I can look into this summer.
                </p>
                <p>
                  For the rest of the afternoon I worked more on my code and prepared what I have so far for my 1:1 with Rajiv tomorrow and for the business procurement team at some point in the future if needed. I decided to take down the times for the algorithms that I was initially considering so that I had a clear reason for choosing LinearSVC. I went back to RandomForestClassifier and decided to try some other parameters, since LinearSVC and RF have similar accuracy levels. I remember reading about how having a large number of trees can greatly impact the run time, so I tried RF on the original 200 trees, then cut it to 100, then down to 25. At 25 trees, RF takes about 3:30 minutes to run, which is a lot faster than the original 25 minutes to run at 200 trees. However, this is still considerably slower than LinearSVC which only takes 1:45 minutes to run. I then ran SGDClassifier, which had a comparable runtime to LinearSVC on smaller samples of about 200 rows, but when I increased the sample size it was even slower than LinearSVC. According to quite a few sources, LinearSVC seems to be more suitable to larger scale samples than SGDClassifier. With this analysis done, I feel solid in my choice of LinearSVC for the model’s algorithm in order to maximize both accuracy and efficiency.
                </p>
                <p>
                  I’m still looking into LinearSVC parameters, but I haven’t come up with anything that seems to fit the data that I am using. I may need to rework the structure of my data or keep looking for algorithms to get a meaningful increase in accuracy while maintaining the current level of efficiency. For now, I’m going to keep looking, but reworking the structure of the data is an option that I will keep in mind. I also finally had the opportunity to look into the source that David recommended last week. A lot of the information confirms the choices I have made with SVC and text classification, but it also gave me some new things to think about like using word2vec instead of tf-idf or looking into more deep learning/unsupervised options. I’ll look into these more during my free time and see if I can feasibly work these aspects into my existing structure.
                </p>
                <p>
                  I was scheduled to have my 1:1 with Nancy today, so I compiled some questions about career focus and mobility in tech, but this seems to be a busy week for her, so we’ve rescheduled for Wednesday morning. I also compiled some notes about the broad view of what I’ve accomplished so far, demonstrations of outputs, and further questions for my 1:1 with Rajiv tomorrow. I’ll also continue looking into ways to expand my code using the topics in David’s article or LinearSVC parameters tomorrow. I’m looking forward to meeting with Rajiv to see if he has feedback for next steps I can take.
                </p>
            </div>
          </div>

          <div id= "June-26-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 26, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Friday</p>
                </div>
                <div class="post-title">
                    <p>Ready For Next Steps</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I didn’t have any meetings today, so I was able to focus on my code and gain some insight on where to go next. This morning, I worked on formatting my outputs. I decided to have separate spreadsheets for the ‘UNSPECIFIED’ predicted outputs and comparing the predictions for the randomly sampled defined output to their actual values. I also included some conditional formatting in the defined output page to more easily spot where my predictions did not match the actual values. I decided to run both the ‘UNSPECIFIED’ code and the defined prediction code on 200, 2000, 20k, and the total 200k data set to see how the predictions improved as the data set got larger. It’s a bit hard to make concrete observations on the ‘UNSPECIFIED’ data but it seems like the data is better categorized for a larger data set. However, there are definitely still misclassifications. For example, there are a couple instances where software gets classified as hardware. For the defined outputs, the predictions are definitely better with a larger data set. A data set of 200 is about 93% accurate and a data set of 200k is about 98% accurate. However, with my current method of ‘random sampling’ some of this data is the training data, so the results are misleadingly high. Next week I may look into asking for more data or splitting off a section of the sample data specifically for testing, which would not be included in the training data.
                </p>
                <p>
                  I also captured some visualizations of where the ‘UNSPECIFIED’ predictions were getting categorized. I wanted to see if this matched the distribution of the defined data set to see if the imbalance of the training data set was affecting the predictions. From what I can tell, it doesn’t seem like the imbalance is affecting the predictions too much, but if incorrect predictions into the same categories continue, I will look further into this.
                </p>
                <p>
                  In the afternoon, I worked on improving the accuracy of my code. I looked into the parameters for LinearSVC, but it doesn’t seem like I can feasibly implement a hierarchy system for classifying the data without sacrificing a lot of efficiency. While looking into parameters I stumbled across a method called SGDClassifier, which is for Stochastic Gradient Descent. This uses the same logic as Linear SVMs but using gradient descent to find the optimal parameters. I implemented a version of SGDClassifier using its default parameters and it seems to be almost equally efficient compared to LinearSVC. I’ll need to look more into how I can possibly change the parameters for both LinearSVC and SGDClassifier to improve the accuracy. 
                </p>
                <p>
                  I think that I currently have a pretty efficient and accurate prototype, where it is 95% accurate on a 35,000 row training data set and can make predictions for 200,000 rows in just over 2 minutes. Of course, these can both have room for improvement, but I would like to check in and get some feedback on where I am before pouring time into small accuracy and efficiency changes, in case I need to make any larger adjustments. I might ask Rajiv or Kiran what they think on Monday. Also, since I have the output method set up, I’m wondering if I should prepare to show the business team where I am so far or if I should make more progress regarding scalability before I demo my code to them. I’ll ask Rajiv about this on Monday. Since the machine that Himanshu used to create his server was decommissioned, I needed new resources for setting up a server. After asking Rajiv for additional resources, he directed me to Don, a Systems Engineer. I haven’t had the chance to send out a message to him yet, but I plan to do this first thing Monday morning.
                </p>
                <p>
                  I’m honestly a bit surprised at the progress that I’ve been able to make this week and having great connections like Kiran, Rajiv, David, and Nancy has definitely helped a lot. As I get further into the project, I anticipate that there will be less robust resources and more challenges, since I am still in the initial building phase of the project so far. I’m looking forward to learning more about servers and showing others the progress that I’ve made. :)
                </p>
            </div>
          </div>
          
          <div id= "June-25-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 25, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Thursday</p>
                </div>
                <div class="post-title">
                    <p>Writing Outputs</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I’m definitely getting more used to scrum calls :). This morning, I understood a lot more of the terminology and was able to follow along with the general flow of the meeting. However, I am still a bit lost during the more in-depth discussion of certain components. As I listen in to the calls and get more familiar with the components hopefully I will be able to understand more of these in-depth conversations as well, but it’s nice not to feel completely lost anymore.
                </p>
                <p>
                  This morning I worked on choosing an algorithm to stick with. As I mentioned yesterday, I split LinearSVC and RandomForestClassifier into separate documents so that could compare their speeds. LinearSVC is definitively faster than RandomForestClassifier, so I will be using LinearSVC as my algorithm for finding predictions from now on. I also captured the images of data visualizations from yesterday’s code in case I want to view them or display them during meetings so that I don’t have to re-run my code. It’s actually pretty astounding looking at the graphs and seeing how much of the sample data is unspecified! (~70%).
                </p>
                <p>
                  In the afternoon there was a 2 hour workshop about the economics of Fidelity. This workshop was meant especially for interns to learn more about the inner financial workings of how Fidelity operates. Some of the content was familiar from the Financial Economics and Macroeconomics courses that I took in school, but there was also a lot to learn. Especially interesting to me was seeing how the concepts that I learned in Economics classes were applicable to running a business and investing in different assets. My particular project isn’t really involved with investing, but I think that it’s important to know how other parts of Fidelity work even if I am not directly involved in them.
                </p>
                <p>
                  For the rest of the day, I worked on how to write output predictions into Excel. This process was a bit frustrating, since I found that a lot of sources were outdated or certain functions were no longer functional. Eventually, after quite a lot of trial and error, I was able to successfully get my predictions correctly formatted into an Excel sheet. I first thought it would be interesting to make predictions for the ‘UNSPECIFIED’ items, since these items were cleaned out from the original data. Since these items were previously classified as ‘UNSPECIFIED’ I don’t have a way to cross-reference and make sure that these items have been put into the correct category. When I looked at the data, it seemed like most of the categorizations were not wildly inaccurate. However, since I have only been working with this data for about 2 weeks, I don’t have the intuition for where the data should really be classified, so I will check with Kiran or the Business team. During Tuesday’s meeting with Rajiv and Kiran, they mentioned that it might be good to print out the actual vs. the predicted test data, so I spent a little while tweaking my code to have this functionality. Looking at this data, it seems like my code does a pretty good job, but there are some instances where the categorization is completely different than what it should be. I will need to brainstorm and research ways to fix this issue. Currently, I am thinking about a hierarchy or weighting, since some categories are more relevant to classification than others. But I am not sure how feasible this method would be with the algorithm I am using.
                </p>
                <p>
                  Tomorrow, I have almost no meetings! I plan to look into improving the accuracy of my classification tool as mentioned above. Today I spent a lot of time getting my code to function, but I have a lot of random variables or irrelevant comments floating around, so I will also spend some time cleaning up my code and making it more readable tomorrow. If I have time or if I get stuck, I might also start googling or asking around about setting up a Fidelity server and making my tool scalable like Rajiv mentioned. If possible, I will look into getting my code to be more efficient, but after today, I think some of the speed issues were from running so many models at once, so it may not be as big of an issue as I originally thought.
                </p>
            </div>
          </div>

          <div id= "June-24-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 24, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Wednesday</p>
                </div>
                <div class="post-title">
                    <p>More Accurate But Less Accurate</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I signed on a bit earlier this morning and got in touch with Himanshu. He told me that he used to have the Fidelity local set up, but it is currently decommissioned since the machine they were running it on got decommissioned. This is unfortunate, but I think that further down the line, when I have a better prototype, I can look into more resources and see if there are viable options about Fidelity servers. Perhaps I can reach out to my mentor group, especially since Nancy has added more developers to the chat! Himanshu also suggested that I use Spyder instead of Jupyter within the Anaconda suite, since Spyder is better suited for sunning scripts and applications and is more GUI centric so it would be better for my classification tool.
                </p>
                <p>
                  I sat in on the scrum call this morning and things were a lot clearer after talking to Rajiv and Poornima yesterday! I still don’t understand the details of what is being discussed, but I can follow along and get a general idea of what the conversations are about. I heard a few new terms, but I was able to google them after the meeting and answer those questions.
                </p>
                <p>
                  For the rest of the morning and early afternoon, I worked on transferring my code from Jupyter to Spyder. This went pretty smoothly, since they both run Python. I also ran my code on a larger sample size of 20,000 rows. This was pretty slow to run even after I removed some of the graphics being created. Something I will work on tomorrow is creating separate files for running the Linear SVC and the RandomForestClassifier algorithms so that I can evaluate if either of these algorithms is significantly faster than the other. My current code runs all 4 algorithms (Linear SVC, MultinomialNB, Logistic Regression, RandomForestClassifier) in one file, which could be what is making the program so slow. In larger data sets, Linear SVC and RandomForestClassifier are pretty close in accuracy, so if there is a large discrepancy between the algorithms in speed then this will be the determining factor in which algorithm I ultimately choose to run with. The results of running my program on 20,000 rows is promising! I got up to about 97% accuracy, which I am very happy with. :) However, the code for 20,000 rows takes about 6 minutes to run, which I am not happy about. :( 
                </p>
                <p>
                  When examining the graphics for the 20,000 row data set, I noticed that a very large proportion (about 60%) of the data is classified as ‘UNSPECIFIED’ which could throw off the data by a significant amount. I added a few lines of code to filter out all the data which was ‘UNSPECIFIED’ in order to get more representative results. Doing this cleaning brought the accuracy for 20,000 rows down from 97% to 93%. While the accuracy is technically lower, the previous version 97% accurate tool was ‘classifying’ some data as ‘UNSPECIFIED’ which is not very helpful for a classifying tool. So even if the accuracy score is technically lower, the fact that data is actually being classified correctly 93% is still great for now! For fun, I tried running my code on the total 200,000 row sample set. This took a very long time to run (40+ minutes), but I was curious to see how much data was actually unclassified and how accurate I could get. It turns out that out of about 200,000 rows, about 150,000 rows were ‘UNSPECIFIED’ which was really surprising. I was able to get an accuracy level of about 95%. While I was waiting for my code to run, I looked up some more in-depth information about some of the pieces being used within the program such as TF-IDF (Term Frequency, Inverse Document Frequency) vectorization for turning text into vectors to feed into the algorithms and more information on algorithms like Logistic Regression that I am not as familiar with. I added some comments to my code as well and fixed up the formatting.
                </p>
                <p>
                  This afternoon, I also attended a WLG (Women’s Leadership Group) Q&A with Eric Bocan, an Executive Sponsor for the Westlake location’s WLG. He discussed mobility, mentorship, and returning to the office. This wasn’t an event targeted toward interns so it was really interesting to be able to hear his advice for full-time employees and gave me some things to think about for longer-term career goals.
                </p>
                <p>
                  Tomorrow, I will continue working on my code. Specifically, I will split the LinearSVC and RandomForestClassifier algorithms into separate documents so I can evaluate their speeds separately. I will also look more into writing my predictions into an output document. The problem I am running into is that the current method for making predictions doesn't preserve the ids of the objects, so it is hard to map the prediction to the correct row and id in the spreadsheet.
                </p>
            </div>
          </div>

          <div id= "June-23-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 23, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Tuesday</p>
                </div>
                <div class="post-title">
                    <p>Things Are Starting To Make Sense</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I spent most of my day today in and out of meetings. This was kind of a nice change from last week, where I spent most of my time doing individual research. I didn’t make too much progress with my preliminary version of the NLP classification tool, but I got to know what the APT team was working on in a lot more depth, so I still had a full day of learning. I started my morning again with the Scrum call, then almost immediately after had a meeting about an update program. I wasn’t too sure what was going on in these calls since there were a lot of unfamiliar terms and acronyms being thrown around. Since I was confused, I reached out to Poornima to ask if she had time in her schedule for me to ask her some questions about what she was working on, clarify some terms, and give an overview of the workflow.
                </p>
                <p>
                  Rajiv and I also had our weekly 1:1 this morning. He explained more about the inner workings of how Fidelity uses databases and applications of how databases. I think this explanation put into perspective some of the tools that are being discussed in APT meetings. Rajiv even showed some real-time code examples, which was really nice especially since I don’t have experience with databases or SQL. I was able to demo what I worked on yesterday with my 75% accurate tool that compares a few NLP algorithms on the test data set of 200 rows. Rajiv suggested that I show Kiran what I have accomplished so far so I can get some feedback and ask questions, so he set up a meeting for the afternoon.
                </p>
                <p>
                  I had my meeting with Poornima this afternoon. She gave a very comprehensive overview of the Jira workflow and the APT production calendar. She also explained what happens at each stage of the project from start to finish (To-Do, Design, Development, User Acceptance Testing, Pending Migration, Certified), which will help me follow along with understanding the scrum calls. I also asked some questions about some terms and acronyms that I heard but did not understand. A lot of these terms have to do with SQL databases, so I will look more into the terminology on my own time as they come up. Poornima answered a lot of my existing questions about terminology though. I was especially confused about what dependencies, packages, modules, decommissioning, retiring, and schemas were in the context of the team, but it’s much clearer now! Poornima mentioned that when she first joined Fidelity, these terms were also very new to her and how it was almost like picking up a new language. This made me feel a lot better about being lost and asking so many questions and encouraged me that I might be able to understand the meetings by the end of the summer.
                </p>
                <p>
                  My last meeting today was demo-ing my tool for Kiran and Rajiv. Kiran and Rajiv seem happy with the progress I have made so far and it was nice to get some feedback after working on it mostly alone for the last few days. Some of the feedback mentioned from the meeting was that I might need to look into using a Fidelity local server. Currently I am running my code in Jupyter notebooks as part of the Anaconda suite, but it would be a lot more secure and a lot easier to implement company-wise if I was able to set up a local server. Since I don’t have prior experience with local servers, Rajiv suggested that I reach out to Himanshu, since he implemented a local server for his NLP tool. Further down the line, I would also like to ask Himanshu if he has time to take a look at my tool and give me some feedback based on his NLP expertise, since I am new to NLP and may not be doing things in the most efficient way. Lastly, Kiran suggested that I put my predictions into a formatted spreadsheet before showing it to the business side so that it is more of a finished prototype and we can really see how the tool would look in action. These suggestions are all things I plan to get started on tomorrow.
                </p>
                <p>
                  In between meetings I made some small tweaks to my tool in an effort to improve accuracy. I tried adding additional spreadsheet information so that I am training my tool on information from 5 different columns instead of just 3. This improved the accuracy from about 75% to 80%, which was exciting! I also tried running my data on a 2,000 row data set instead of the 200 row data set that I was initially using. This brought the accuracy up to 88%! :) However, I noticed that when I made the jump from 2000 rows from 200 rows, my program was noticeably slower. Efficiency is definitely something I will have to work on improving before I try to run this model on the large amount of data used in Spend (~200,000 rows). These initial improvements in accuracy is very exciting, but I anticipate that fine-tuning the model is going to take a lot more time and research than just adding information. I anticipate having to look into the algorithm’s inner workings and parameters to do this fine-tuning.
                </p>
                <p>
                  Tomorrow I don’t have many meetings, so I can work more on my code. I still have the warnings about ‘minimum split size’ that I was working with and it seems like this is coming from the fact that my data is relatively imbalanced and some categories have very few data members. I’m not yet sure how to address this issue without just feeding my model more robust training data. I’ll also try to wake up earlier so I can reach Himanshu and ask for his advice on local servers. I’m eager to see if I can understand more of tomorrow’s scrum call now that Poornima and Rajiv have filled me in on some of the high-level concepts and terminology.
                </p>
            </div>
          </div>

          <div id= "June-22-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 22, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Monday</p>
                </div>
                <div class="post-title">
                    <p>Somewhat Functioning First Attempt</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I started off my morning by joining the Scrum call again. I think I’m getting a better idea of what is happening now that I have been attending calls for about a week. Honestly, I’m still pretty lost, especially since my project is a bit disconnected from the database work that they are discussing. I think that if tomorrow’s scrum call is still just as confusing, I will reach out to Poornima and meet with her to ask questions about what the team is working on and what she is working on. Hopefully, this will give me more insight into what the scrum calls are about so that I can follow the calls more closely.
                </p>
                <p>
                  For most of the morning, I was trying to download nltk packages. The nltk library is quite large, so it took over an hour to completely download once I passed the right authentifications. While I was waiting during this time, I watched some videos about neural networks. I understand a bit more about hidden layers and activation functions. The downloads finished before I completed the videos, but I have bookmarked them so that I can watch them if I am waiting around or if I need a break from writing code. :) The Fidelity mentor chat was also pretty active this morning, so I talked to Jenna, Benjamin, Anushka, and some of the people Nancy added on Friday. Kaitlin added her mentee, Chelsea, so it was nice to meet someone new. Someone from the CTG interns made a group chat, so I’m hoping to get to know some interns within my business unit beyond just the introductions we had on Tuesday.
                </p>
                <p>
                  Once the nltk packages finished downloading, I tried getting to work using the libraries. I found a tutorial for using SVM and Naive Bayes on text classification instead of on numbers, which fixed the problem that I was having on Friday with using strings instead of floats. However, this tutorial was not accurate at all when applied to my data. SVM was only about 3% accurate and NB was about 3.5% accurate. I messed around with some parameters to try and see if I could get it working more accurately, but did not have much success.
                </p>
                <p>
                  I took a break from coding and talked to Nancy for a little bit about internships, college, and studying abroad. Since I noticed that a lot of people on my team are male, I wanted to ask Nancy about her experience in tech at Fidelity with the gender ratio. She mentioned that while there are more men than women overall, the ratio depends a lot on which team you are on. She mentioned that she was on a team where all the leadership was women and she mentioned that her LEAP class was pretty evenly distributed between men and women. This was reassuring to me and I think I just happen to be in groups that are more male-dominated. For example, our CTG intern group has about 20 people, but just 5-6 out of the 20 are women. The people that I have been in contact with on my team are also primarily male, with the exception of Janice and Poornima. WISTEM also has an event next week, but unfortunately this conflicts with a meeting. I’m looking forward to talking to Nancy more in the coming weeks and getting her advice on the internship and beyond. :)
                </p>
                <p>
                  Later in the afternoon, I found a different tutorial for Text classification, which used SVM, NB, RF, and logistic regression. This method took a while for me to implement since my data has more moving parts than the sample data. It ended up being worth it, since it’s much more accurate than the tutorial I was working with this morning! Each method is about 60-75% accurate. Of course, I will need to work on getting the model to be much more accurate before the Business group can use it since even 75% accuracy isn’t exactly reliable, but this is a good starting point compared to 3%! This tutorial has a lot more ways to visualize the data, which made the models seem much less ‘black box’ and made it a lot easier for me to see what was going on behind the code. The data set that David sent me last week has about 200,000 rows, but I am starting with 200 rows so that it's more manageable as I work through initial bugs and formatting, but the size of the data is something I will keep in mind as I move forward. There are still some changes I need to make to the tutorial’s parameters in order to fit my data. This is because the data I am working with has a lot of different categories and sometimes categories in the test data are not present in the training data, which means the model doesn’t know what to do with them. The large number of categories also makes the data visualization tools a bit difficult to read, so I may spend some time seeing if I can improve readability. There are some other warnings that I need to work through regarding ‘minimum split size’, but I’m not sure exactly what this means, so this is an area that I need to research more tomorrow as well.
                </p>
                <p>
                  Tomorrow I have my 1:1 with Rajiv, so I plan to update him on what I have been able to accomplish up to this point and what I hope to do next. I will also reach out to Poornima tomorrow so I have a better idea of what’s going on during the scrum meetings and so I can finally meet her face-to-face. I should have reached out to her sooner, but all the new information last week was a bit overwhelming so it totally slipped my mind. In terms of the text classification tool, I plan to address the warnings and research (and maybe even implement?) ways that I can improve accuracy.
                </p>
            </div>
          </div>

          <div id= "June-19-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 19, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Friday</p>
                </div>
                <div class="post-title">
                    <p>Finally Coding</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  Today I finished up my preliminary stage of research. This morning I got in contact with Himanshu and expressed my concerns about SVM working best only with small data sets. He said that SVM should be able to work with the size of the dataset that I am working with, but since I mentioned that I was looking into other algorithms, he recommended that I double check on the accuracy of those algorithms before implementing. Based on accuracy, I think that Random Forest would be preferable to Naive Bayes. However, since he mentioned that SVM still seems feasible, I will try SVM first. If I run into issues with SVM and the large data set, I will keep Random Forest (RF) in mind as a backup option. Since I had a plan for which algorithms I wanted to use, I tried to start coding at the end of the morning, but got stuck since I kept running into HTTP errors when installing Python packages through Anaconda.
                </p>
                <p>
                  In the afternoon, I had a bit of a break from coding and research. I attended the Fidtern meetup just like last week. It still felt a bit stilted since we mostly just introduced ourselves and talked about work, but it was good to see some familiar faces. I also had a short conversation with Korey from University Talent about my internship experience, especially since the internship is virtual this year. Around this time Nancy (my FYPN mentor) introduced us to some other software engineers (Gabriel, Hannah, and Kaitlin) as additional resources and connections. I really enjoyed connecting with more young full-time employees in the setting of a group, since I find it much easier to keep conversation flowing in a group with less pressure put on each individual person. Nancy set up a fun escape room activity with Anushka, Jenna, Benjamin, Kaitlin, herself, and I. We solved puzzles together virtually and I thought it was a cool interactive way to bond. :) At the end of the call, when we were talking about ‘highs and lows’ of our week, I mentioned that I was having trouble installing packages. Nancy connected me with a Senior software developer, Josh, on her team to help me out. Within about 10 minutes he was able to help me fix my issue and request elevated access so I wouldn’t have to worry about these problems in the future! It was really nice to get such personal and immediate help from a senior developer. I think this experience has encouraged me to reach out earlier when I run into issues. I was struggling for over an hour with this issue, but Josh and Nancy were able to help me fix it within 10 minutes!
                </p>
                <p>
                  With this problem fixed, I got into writing code in Jupyter notebooks. I learned how to read data from Excel to Python and how to explore some features of the data using built-in functions. I then tried to run this data through the SVM model. It took me a while to understand what the different pieces were doing, but I learned a lot through the process. However, I got stuck, since the data I am working with is not numerical. My data is categorized based on strings, whereas all of the SVM examples that I have found online are using floats, so I will have to find some workaround. I tried using RF as well, since I thought that RF would be less reliant on floats and it might be easier to make alterations to get it to work with my strings, but so far I have not had much success.
                </p>
                <p>
                  Poornima also reached out to me to check in and see how my first week was going. Rajiv has paired me with Poornima if I have questions and Rajiv is not available since Poornima is relatively new to Fidelity and so I might be able to relate to her experiences more. This week I have not had too many issues, since I have been mostly independently learning about NLP and ML concepts. I think that I get further into coding, I will probably be reaching out to her more as I run into questions.
                </p>
                <p>
                  Next week, I hope to continue working on getting either the SVM or RF model (or both?)  working on my small test set of data. If I can get this working, then I can see what happens to the accuracy as I apply it on a larger scale and make some alterations from there. This first week has been pretty fun and I’ve learned a lot of new information. I’m looking forward to taking the weekend to process the new information and experiences and hopefully next week I’ll be doing more coding and continue making connections!
                </p>
            </div>
          </div>

          <div id= "June-18-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 18, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Thursday</p>
                </div>
                <div class="post-title">
                    <p>Exploring Algorithms</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  Today was a bit of a roller coaster when it comes to where my research has taken me. I spent my morning learning about the Support Vector Machines (SVM) algorithm that Himanshu mentioned yesterday. A brief summary of what I learned is that SVMs plot data into an N-dimensional space, then the algorithm creates a hyperplane that categorizes the data appropriately. This is a very popular ML supervised algorithm and is preferred to a lot of other classification models because of the kernel function, which can map the data to higher dimensions in order to better fit the data if the initial dimensions don’t allow for the data to be categorized well enough by the existing Support Vector Classifiers. I followed a toy example for using SVMs to classify if a recipe was for cupcakes, muffins, or scones based on the percentage of butter and sugar in the recipe. (Apparently cupcakes have pretty high amounts of both butter and sugar, muffins have low amounts of both, and scones have a lot of butter but not a lot of sugar). 
                </p>
                <p>
                  The SVM algorithm seemed to have a lot of promising qualities, especially since it was not too computationally or memory expensive and had high accuracy. However, upon reading more detailed documentation and articles,  quite a few sources mentioned that it works best with smaller data sets. From what I understand, the data I am working on does not seem to be a small data set, so I'm not sure if SVM is the best algorithm. I looked into other algorithms used for classification. The two that seem most promising are Naive Bayes and Random Forest. (Other algorithms like Tree Decision and K-Nearest Neighbors don’t seem right for this project based on incompatibility with accuracy or categorical data).
                </p>
                <p>
                  Naive Bayes has the pros of requiring less computational power, accurately working on large datasets, being fast, being pretty easy to implement (supposedly), and being best suited for text classification so it could be used for the sentiment analysis application that David mentioned last time. However, some tradeoffs are that it makes the strong assumption about the features to be independent and implements zero frequency, which means if the category of any categorical variable is not seen in training data set then model assigns a zero probability to that category and then a prediction cannot be made, so might not be able to fulfill the 'flexible and dynamic' part. Random Forest has the pros of having high accuracy, flexibility, and less variance. Also it’s supposedly not too difficult to implement, works well in handling missing values and detecting outliers, and can identify the most important feature among available features (not sure how relevant this is for the current project). However, it has high computational and memory cost. Additionally, as the number of trees increases, the algorithm can be slow. So, it's not necessarily a problem if the data set itself is large (i.e. a lot of rows in the spreadsheet), but it could be a problem if there are a lot of categories to classify the data into. I’ll ask about the order of magnitude of categories tomorrow!
                </p>
                <p>
                  I spent a little bit more time looking at Deep Learning concepts, but I think my priority is learning these categorization algorithms for now. Tomorrow, I plan to ‘get to work’ earlier in the morning so I can ask Himanshu what his opinion is for SMV if my data set is of a larger scale or if he has additional suggestions for algorithms that I should look into.
                </p>
                <p>
                  This morning, I also had a short meeting with Janice who is the Vice President of IT Management. We talked about my goals for this internship and she gave me a lot of advice and insight about Fidelity as a place to consider working full time. She emphasized how Fidelity cares for its employees through benefits, a flexible environment, and lots of opportunities for mobility within the company. She suggested some ways to get more involved with the culture as well. This has definitely given me a lot to think about when considering if I’ll consider Fidelity as a future employer! :)
                </p>
                <p>
                  I sent an email to David, Cameron, Daniel, Anuj, Kiran, and Rajiv about the current rule-based system for classifying data, so hopefully I’ll get a response tomorrow to clarify those questions and help me find the best suited algorithm for this project! I also have a check-in with Korey from University Talent to talk about how the internship is going so far, so it’ll be nice to catch up with her. I plan to spend tomorrow looking more into Random Forest and Naive Bayes from some different sources. I keep seeing Principal Component Analysis (PCA) mentioned as an algorithm for data analysis and predictions, so I plan to look into if that’s another feasible algorithm tomorrow. Another full day of research and learning new things ahead!
                </p>
            </div>
          </div>

          <div id= "June-17-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 17, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Wednesday</p>
                </div>
                <div class="post-title">
                    <p>Old and New Directions</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I started my morning off by joining the Scrum Meeting as Rajiv suggested yesterday. After what Rajiv explained to me on Monday, I understand a bit more of what is happening at a conceptual level with the way that the Agile method works and Sprints and the daily updates. However, I am still quite lost when it comes to the actual tools and modules being discussed. I’m sure that as time goes on and I attend more of these meetings it will become less confusing. For now, I will try and keep attending these meetings to become more familiar with the workflow within Fidelity.
                </p>
                <p>
                  Later in the morning, I met with Himanshu and Rajiv to talk about my project. Himanshu is part of the team in India, and he has worked with NLP in the past, so he was able to offer a lot of guidance and helpful information when it came to pointing me in a more focused direction for my research. In particular, he suggested that I check out the Support Vector Machines (SVM) library, since this library is often used for categorization. He also advised that I will probably need to use custom logic for the machine to be able to process Fidelity specific data since pre-existing data sets like NLTK might not be specific enough to cover the vocabulary used in Spend. We talked a little about unsupervised and supervised learning, since it seems that the Business team wants a model that uses unsupervised learning for their flexible and dynamic model. Since Himanshu is in India, my work hours are late into the night for him. To communicate with him in real time, I’ll wake up about an hour earlier, otherwise he suggested that I leave messages for him at the end of my work day and he will respond to them in his morning hours. I’m looking forward to hearing more about the insights he gained from his NLP project in the future and applying it to my own project!
                </p>
                <p>
                  I finished the introductory neural network course that I started yesterday during the rest of the morning and into the early afternoon. I think this course is a good starting point, but if I were to implement deep learning, I anticipate that I would need to find more resources about how to utilize the neural networks and other deep learning concepts that were mentioned in the tutorial.
                </p>
                <p>
                  After lunch, there was a Fidtern workshop about personal branding. I thought this workshop was pretty interesting. Although the workshop covered a lot of material that I had already learned from Mudd’s career workshops, it was interesting to hear it from another viewpoint. I noticed that there was an emphasis on soft skills during this workshop, which is something I can work on displaying more in interviews and on my resume. Around this time I also had some conversations about TV and movies with my fellow interns from FYPN. It seems that as we all get busier, people have less time to talk, but I hope that we can still keep up these conversations in whatever free time we find!
                </p>
                <p>
                  For the remainder of the work day, I started the Stanford NLP and Deep Learning course and started looking into the SVM library that Himanshu recommended. While the Stanford course is very informative, I think that it goes more into the math behind the tools than I am looking for. As ChunLei mentioned earlier this week, my focus for this project shouldn’t be on the algorithms portion of Machine Learning, but rather how these ML algorithms are applied. Of course, to apply the algorithms, I should understand how they work to some extent, but since the Stanford course seems to focus more on the inner workings of the algorithms than the applications, I don’t think I will be continuing with the Stanford course since it doesn;t align with my goals. I also started looking into the documentation for SVM, and I think I understand the basic ideas. Tomorrow I will dive deeper into fully understanding the library. SVM seems much more suited to my project than the NER library that I found earlier this week since it focuses more on categorization based on word semantics than categorization based on sentence structure. :)
                </p>
                <p>
                  I also started installing the necessary tools for starting to write some code, but didn’t have time to finish setting it up, so I will continue to do this tomorrow. I’m not familiar with the Fidelity systems and what restrictions there are on downloads, so I’m trying to take this part slowly and carefully to avoid running into trouble later. Tomorrow, in addition to more research and installations, I also have my meeting with Janice to look forward to.
                </p>
            </div>
          </div>

          <div id= "June-16-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 16, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Tuesday</p>
                </div>
                <div class="post-title">
                    <p>Networking: Neural and Otherwise</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  Today I spent more time collecting resources and learning about NLP. In particular, today I started looking at resources about Neural Networks. I found an introductory Neural Networks tutorial on Youtube, which requires no prior knowledge. I started working through this course and plan to use this as my jumping-off point before diving into more advanced resources. I found other courses and articles about Neural Networks and NLP. In particular, I found entire courses from Stanford, one of which is about NLP specifically and another which is specifically about NLP with Deep Learning. I think that once I finish the introductory neural network course I will try the Stanford NLP with Deep Learning course, and if it is still too advanced to follow and I will go back to more basic courses. I also found a few more resources about NER. In particular, I started looking into the documentation for spaCy. I only have a basic overview, so I will go back later and read more of the details when it comes time to start building the tool.
                </p>
                <p>
                  Today I also looked into some possible packages that I might use in the process of pre-processing the text,, such as BeautifulSoup and NLTK. I looked at the capabilities of these tools and how I might use them in practice. In the coming days, I will look into more packages for feature extraction and classification, since I anticipate that this will be the bulk of the work when it comes to being able to accurately classify the Spend data. In the past two days there has definitely been an overwhelming amount of new information, but writing it down here and trying to absorb it little by little has definitely been helping. :)
                </p>
                <p>
                  Today I also got to contact a few different groups of people. So while I am doing quite a bit of individual research, I am still able to reach out to Rajiv and other team members as well as other interns.
                </p>
                <p>
                  This morning Rajiv and I had our first weekly 1:1. During this time, I got to talk about my personal career aspirations and what I was hoping to gain from the internship. I talked about my experience working with front-end and back-end and how I was interested in ML and AI, but there didn’t seem to be many opportunities in ML for undergraduates. Rajiv offered some helpful insight about how fast the CS industry moves and how in 2 years when I graduate, ML might be much more prevalent and how job opportunities could very well open up for undergraduates like me. Rajiv also taught me about Cloud products, specifically SAS, and how they are integrated into Fidelity as well as our everyday lives. I enjoy learning about current relevant technologies used by Fidelity during these meetings with Rajiv. He also suggested that I come to more Scrum meetings so that I can learn more about the corporate workflow, specifically with Agile. I will definitely take him up on that offer and I plan to attend tomorrow’s Scrum meeting!
                </p>
                <p>
                  This afternoon, we had a meet-up with CTG leadership and interns. The meeting was pretty casual, and we went around introducing ourselves and having some conversations about the COVID-19 situation. It was nice to be able to meet the other interns that I might have been working alongside or even sharing office space with in Westlake. I connected with some interns on LinkedIn after the meeting and one intern reached out to me and we had a brief conversation about our projects. It’s cool to hear about other interns’ projects and have the “water cooler conversations” that we might have had back in the office. I also kept in touch with my mentor group throughout the day.
                </p>
                <p>
                  A couple of questions came up during my research from yesterday and this morning, so I messaged all the members of yesterday’s meeting. David responded very promptly and answered a lot of my questions and even gave me some sample data to look at! At first glance, the data is pretty overwhelming. I don’t fully understand it yet, but I will take a better look at it once I get more involved with the specifics of the project. Rajiv also informed me that Janice invited me to Thursday’s FPAIT staff meeting, where I will introduce myself. I have yet to receive the email invite, but I am looking forward to meeting even more people!
                </p>
                <p>
                  Tomorrow I plan to continue my introductory neural network course and hopefully start the Stanford course. Additionally, Rajiv has set up a meeting for me to introduce myself to a member of the team from India after the morning Scrum meeting, which I am excited about. There is also a Fidtern workshop about professional branding that I hope to attend in the afternoon. Looking forward to another busy day of learning!
                </p>
            </div>
          </div>

          <div id= "June-15-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 15, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Monday</p>
                </div>
                <div class="post-title">
                    <p>Focusing My Research</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I spent most of my time today independently researching the more basic concepts of NLP. and the general workflow of data science. I found a particularly helpful video was this video from PyOhio about NLP in Python. The speaker was Alice Zhao and I found that the way she explained the concepts was very clear and approachable for someone like me who is new to NLP and Data Analytics as a whole. I spent most of my morning watching this lecture and taking some notes to come back to later. From this video I gained a basic understanding of the process of NLP and the kinds of insights that can be drawn, but I also found that there were some topics that I was interested in looking further into. For example, I would like to look more into using Pandas, python packages, and stemming.
                </p>
                <p>
                  In the early afternoon, I also looked into other resources, such as the ones that ChunLei sent me on Friday. I looked into resources like Vik’s Blog, towards data science, and sentdex. These resources brought up some very interesting concepts that I had not seen before, and which I will definitely need to look more into. From Vik’s Blog, I think I will research more about Linear Regression, since this is a concept that has come up in my Scientific Computing course, but I would need to do more research to see how exactly it is being applied to NLP and this project. From toward data science, I am interested in doing more research about what tool might be best suited for this project. I know that Python was mentioned before, but it may also be worth looking into R and TensorFlow. I also learned about the difference between the classical ML model and Deep Learning. Classical ML is where you train the model using feature extraction on the input and then classification on the extracted features. Deep Learning is where the feature extraction and classification happen together in one step using neural networks. From sentdex, I will definitely look more into NLTK, since it seems to have some extremely useful built-in tools. 
                </p>
                <p>
                  ChunLei directed me to a site called Amazon SageMaker. He recommended that I not worry too much about the math and the details of the project, but rather get a better understanding of high level concepts. After reading through the documentation, I found that one particular feature that might be useful for this project is Named Entity Recognition or NER. This feature sifts through text data to locate phrases and categorize them into labels like person, organization, or brand. I think that this labelling could be extremely useful for Spend Categorization. However, one problem is that based on the resources that I have seen so far, it seems that NER is used in sentences, whereas I am not sure what form our data will look like. Another problem is that NER categorizes sequences based on predefined labels, which would not be cohesive with the desire for a more dynamic model. NER is definitely a topic that I will spend time researching more.
                </p>
                <p>
                  Later, I had a meeting with Rahiv, Kiran, Cameron, and some new faces including Daniel and David from Business, and Anuj from Kiran’s team. We talked more about the expectations of this project and the pacing. David offered some insight about stemming vs. lemmatization, and suggested that lemmatization might be more accurate. I agree with him and would definitely implement this approach when it comes to that point in the project. I asked some questions about if previous projects have used classical ML or deep learning, but it seemed that the decision of which is best suited for this project will be left up to me. We ran out of time during this meeting since there was a lot to discuss, but I am looking forward to sharing more about what I have learned and asking more questions in the future.
                </p>
                <p>
                  At the end of the workday, I started looking more into classical ML vs Deep Learning. It seems like classical ML is much more approachable and may be easier to implement, but Deep Learning will probably do a better job of accomplishing the dynamic and self-learning model that Business is asking for. I need to do more research about Deep Learning and specifically Neural Networks to see if this is something that I would realistically be able to learn and get started on during the summer. The best and most efficient ways are always the hardest to implement :,). 
                </p>
                <p>
                  Unrelated to work, I found out that I have some connections to other Fidterns! An intern in my mentor group, Benjamin, knows a friend from high school, and another SWE Fidtern, Cassie, knows one of my friends from Mudd! Additionally, I’ve continued talking to my mentor group, and today Nancy added a new member, Jenna, who shared pictures of her adorable dog. :)
                </p>
                <p>
                  Tomorrow, I plan to start looking into some of the topics that I came across today, particularly classical ML vs Deep Learning, Neural Networks, NER, Automatic Feature Extraction, and word2vec. I don’t anticipate that I will be able to get to all of these topics tomorrow, but it’s a starting point into more research! Tomorrow there is also a CTG intern meet-up in the afternoon, where I’m looking forward to meeting other interns in my business unit. 
                </p>
            </div>
          </div>

          <div id= "June-12-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 12, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Friday</p>
                </div>
                <div class="post-title">
                    <p>The More I Learn, The More Questions I Have</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  Today was the last day of webinars! It was nice to learn so much during training, but honestly, I think I’m ready for a change of pace. Sitting through long Zoom meetings is getting quite exhausting and I’m looking forward to learning more technical skills through projects. This morning we had a pretty information-heavy webinar about HR and specifically about the protocol for logging the hours we work. Then in the early afternoon we played some fun icebreaker games. At noon we had an intern meet-and-greet session where I got to know some interns from Massachusetts and New Hampshire.
                </p>
                <p>
                  I had a meeting with Kiran today to update him on what Cameron and I talked about yesterday. Kiran was able to provide me with a lot more detail about the project’s current status and what my role on the project might be. He also put me in contact with an NLP specialist, ChunLei. ChunLei was able to explain a lot of insight about NLP and how it would play into what the Business team was asking for. ChunLei also pointed me to some really great introductory resources that I was able to get started with.
                </p>
                <p>
                  For the rest of the afternoon, I looked into the resources ChunLei recommended as well as some resources that I found on my own during my online research. I have learned a lot of introductory material about topics like NLP, Deep Learning, Tokenizing, etc. In the coming week or so, I anticipate that I will be spending a lot of time self-studying these topics and getting more familiar with NLP at a high conceptual level. I have found that the more I learn about these topics, the more questions I have for the Business team regarding what they expect for their new Spend tool. For example, I am wondering what kind of data I will specifically be working with and how I might access the data if it is from 3rd party sources. I am also wondering about the new categorization aspect. What would necessitate new categorization? How specific would the categorizations be? How generally can we lump items together? Lastly, I would wonder if there is a strong preference for Python because of previous infrastructure or ease of implementation into the existing tools or if this preference is due to the existing packages? I will definitely try and ask these questions in Monday’s meeting with the Business team and with Kiran.
                </p>
                <p>
                  Throughout the day, I also remained in contact with my mentor group, which was a lot of fun! Next week, I hope to keep in contact with the interns I have met this week and to keep learning about new topics like NLP. I am still getting used to the corporate structure and making sure that I am on top of my meetings.
                </p>
            </div>
          </div>

          <div id= "June-11-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 11, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Thursday</p>
                </div>
                <div class="post-title">
                    <p>A Potential Project!</p>
                </div>
            </div>
            <div class="post-body">
              <p>
                This morning, like the 3 mornings before, consisted of more webinars. Today we learned about strategies we can use to make the most of our internships, like making connections and getting involved in different groups. Later, we heard from FYPN (Fidelity Young Professionals Network) and some details about the mentorship program. Coincidentally, my mentor reached out to me today and I got to meet her for the first time (more on that later).
              </p>
              <p>
                In the afternoon, I had my meeting with Cameron from Procurement Business Analytics. Unfortunately, Daniel had a last-minute conflict and could not make it to the meeting. I got to introduce myself to Cameron and hear about the project that I might be able to get involved with. From his explanation, I think I would be interested in joining this project. I’ll try and explain the main ideas of what I learned, but there was quite a lot of new information to absorb. Procurement is a department that deals with purchasing items for Fidelity, which can include anything from software and hardware equipment to the desks and chairs used in office. Procurement has to classify these purchases into the appropriate categories to log their spending. The current model is using a rule-based system. However this model is not very flexible, so the project I have the opportunity to get involved with would classify the spending using NLP (Natural Language Processing) using past data of similar spend and possibly enrich the data with third parties to align with industry changes. I’m not totally sure what this means quite yet and some of the terminology is confusing, but I will definitely do some research so that I can come up with more specific questions to ask at the next meeting! I haven’t worked with NLP before this, so I will definitely have a lot of new things to learn and catch up with.
              </p>
              <p>
                Cameron mentioned that this project is in its very early stages of planning, so on the bright side, this means that I might not have as much catching up to do in regard to this project before I get involved. I’m not quite sure who exactly I would be working with, but since I am new to NLP, I would love to have some guidance or specific people I can reach out to with NLP questions. I think this would make the learning process a lot more comfortable and allow me to make more connections. :) Tomorrow I will meet with Kiran and Rajiv to update them on what I talked to Cameron about.
              </p>
              <p>
                This afternoon, I also met with my FYPN mentor, Nancy! We talked for about half an hour mostly about non-work related things, which was a nice break. I was added into a group chat with two other interns who are Nancy’s mentees, Anushka and Benjamin. It was nice to be able to connect with other interns and hear about the experiences of someone newer to Fidelity. I look forward to talking to them more throughout the next couple weeks.
              </p>
              <p>
                Tomorrow is the last day of training! I’m looking forward to getting started on projects and learning as well as connecting with other interns.
              </p>
            </div>
          </div>

          <div id= "June-10-2020-FI">
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 10, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Wednesday</p>
                </div>
                <div class="post-title">
                    <p>Diversifying Connections</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  I started off my morning by joining the APT Scrum meeting. I got to meet the Accounting Dev Team as well as some of the India team. As mentioned yesterday, the Scrum call was a daily status call where the team updates each other on the progress they are making and brings up any problems that they are facing. Specifically, I got to hear from Ramya, the Scrum Master for APT. I got a rough idea of the workflow within the team, but to be completely honest, I had no idea what was going on in terms of the projects that were being discussed. It was a bit intimidating to have so many new concepts swirling around, but on the bright side, this means that I’ll be able to learn a lot of new things!
                </p>
                <p>
                  After the Scrum meeting was the start of training for the day. The majority of training today was about ERGs, which are Employee Resource Groups. We got to hear from the heads of many groups such as Fidelity Pride, AERG (Asian Employee Resource Group), FiVe (Fidelity Veteran Employees), WLG (Women’s Leadership Group), Fidelity Enable, and WITSIG (Women in Technology Special Interest Group). This webinar was quite long, but it was a good way to hear the insights from Fidelity members across many different business groups. After the session I signed up to join AERG, WLG, and WITSIG. I’m looking forward to getting involved and meeting new people from different fields via these groups. :)
                </p>
                <p>
                  Later we had a shorter webinar from Fidelity Cares, which told us about how Fidelity is involved in local communities. As part of the webinar, we participated in a project where we recorded ourselves reading children's books to be sent to local Boys and Girls Clubs across the country.
                </p>
                <p>
                  In the afternoon, I met with Rajiv to talk about his experience at Fidelity as well as to learn more about what happened in the Scrum meeting from the morning. I got to hear about Rajiv’s journey from being an international student studying electrical engineering to getting involved in databases via Oracle and finally working at Fidelity. I also learned a lot about the Waterfall and Agile methodologies. From what I understand, the Waterfall method includes a lot of planning and iterative testing before developments are rolled out. On the other hand, the Agile method includes more frequent deployments based on input from the user and follows the ideology of “pacing over perfection.” Rajiv explained to me that APT uses Agile, with Scrum meetings as a way to check up on the individual members of the team and progress they are making or the problems they are facing. This definitely cleared up a lot of what I was wondering about the workflow and team dynamics at a high/conceptual level, but there is plenty more for me to learn about the process and dynamics of the team. Perhaps I will read up on Agile and Scrum in my free time this week!
                </p>
                <p>
                  Lastly, Rajiv asked me to set up a meeting with Cameron from Procurement Business Analytics. I sent a Zoom invitation to him for 3pm EST tomorrow, where I will be meeting with him as well as Daniel. This meeting is just for me to introduce myself, since it seems like I may be working with them during my internship, as well as get an idea of what their team is working on.
                </p>
                <p>
                  Looking forward to more training and meeting new people tomorrow!
                </p>
            </div>
          </div>

          <div id= "June-09-2020-FI">            
            <div class="post-header">
                <div class="post-date">
                    <p><strong>JUNE 9, 2020</strong></p>
                </div>
                <div class="post-dow">
                    <p>Tuesday</p>
                </div>
                <div class="post-title">
                    <p>Getting Used To It All</p>
                </div>
            </div>
            <div class="post-body">
                <p>
                  Today was pretty similar to yesterday in terms of having mostly webinar training and getting to know the team that I will be working with this summer a bit better. I think I am getting more comfortable with the platforms that Fidelity uses to communicate across the firm, but there is still quite a lot to get used to over the next week. I signed up to be on Digital Champions on Yammer and posted an introduction, which I hope will help me connect me with other interns this summer.
                </p>
                <p>
                  My morning started with a few webinars. We had a brief overview of Fidelity’s history, structure, and financial services. I thought this was a good way to contextualize the work that we will be doing in the upcoming weeks. Some of the terms mentioned were familiar to me from the Econ courses I’ve taken, but there was plenty of information that was very new to me! The next two webinars were about the technology tools that Fidelity uses. We learned about the different roles of Yammer, Microsoft Teams, Skype, Fidelity Central, Workday, Outlook, and Zoom. While this helped me understand the tools, I am still a bit overwhelmed with the amount of locations to check information on. I think I will become more familiar with this software as I continue to use it.
                </p>
                <p>
                  Later in the afternoon, I got to talk to my manager, Rajiv, and meet Kiran, who is in charge of procurement. I think that I will probably be working closely with Kiran’s team since they work primarily in Python, which is one of my strongest languages. It was nice to talk to Rajiv and Kiran and hear about their experiences at Fidelity. Based on what I’ve heard Fidelity seems to have a flexible and balanced work environment!. I will also be attending tomorrow morning’s daily scrum meeting, just to listen in and meet more of the team. :) I look forward to hearing about what my mentors/coworkers have been working on and getting to work alongside them and learn skills from them throughout the internship.
                </p>
                <p>
                  Tomorrow’s training will likely include more webinars and getting familiar with Fidelity’s technology and culture. I’ll try and keep active on Yammer and engage with the team I will be working with, even though we are virtual!
                </p>
            </div>
          </div>

            <div id= "June-08-2020-FI">
                <div class="post-header">
                    <div class="post-date">
                        <p><strong>JUNE 8, 2020</strong></p>
                    </div>
                    <div class="post-dow">
                        <p>Monday</p>
                    </div>
                    <div class="post-title">
                        <p>First Day As A Fidtern!</p>
                    </div>
                </div>
                <div class="post-body">
                    <p>
                      Today was my first day as an intern! This week will primarily be training via online webinars and some meetings with my manager, but I am excited to start learning more about Fidelity and the role that I can play within the firm.
                    </p>
                    <p>
                      I started the morning off by setting up my work laptop. I received two laptops and I am not sure if this was intentional and I will be using both or if this was a mistake. I have reached out to the University Talent team to ask about this issue. I then spent about 2 hours going through my Fidelity email’s inbox and getting acquainted with the different platforms that the firm uses, such as Yammer, Microsoft Teams, Zoom, etc. I found it interesting that Fidelity uses so many different platforms. I am still a bit overwhelmed with trying to keep up with all the platforms, but I anticipate that there is a high learning curve and I will get used to using all these platforms to connect with my new co-workers! :)
                    </p>
                    <p>
                      At 10am we moved onto the webinars. Today’s webinars included a welcome webinar which welcomed all interns to the firm, a Cyber Safety webinar which educated interns about how to protect ourselves and Fidelity from cyberattacks, and a Intentional Connections webinar which addressed how to create meaningful personal relationships while working remotely. I found these webinars pretty informative and I especially enjoyed the Intentional Connections webinar, since I find it a bit more difficult to communicate with people virtually instead of in-person. I am looking forward to more useful information via webinar throughout the week!
                    </p>
                    <p>
                      I was also able to get in contact with my manager (Rajiv) this morning. He introduced me to another member of the team and discussed what I might expect from the internship. It seems that I will be on the Oracle team. Rajiv mentioned that they work closely together with the procurement team, so my role may include being a liaison between the teams. He specifically mentioned that I might be doing some documentation and working with Python and Java during my internship. I am looking forward to learning more about Agile and development releases, since Rajiv mentioned these two areas which I don’t have much previous experience with. This seems very different from my CS experience last summer and I am looking forward to learning a lot of new CS applications through this new experience!
                    </p>
                    <p>
                      Tomorrow holds another full day of webinars and hopefully I can meet more of my fellow interns as well as getting to know the team I will be working with this summer!
                    </p>
                </div>
            </div>
        </div>

        <div class="proj-footer">
            <p>
                This summer I am working as a Software Engineer Intern as part of the CTG (Corporate Technology Group) business group. Specifically, I am working as part of the FPAIT (Finance Procurement Accounting Infrastructure Team).
                <br> I am working in close partnership with the Business Procurement Team to implement NLP into their Spend classification process so that it is more flexible and requires less manual upkeeping than the current rule-based model requires.
                <br> <em> More details to come as I settle into my internship role!</em>
            </p>
        </div>
    </div>

    <!-- JavaScript for Navigation Bar-->
    <script>
      function openNav() {
        document.getElementById("mySidebar").style.width = "250px";
        document.getElementById("main").style.marginLeft = "250px";
      }
      
      function closeNav() {
        document.getElementById("mySidebar").style.width = "0";
        document.getElementById("main").style.marginLeft= "0";
      }

      window.onscroll = function() {myFunction()};
      var header = document.getElementById("myHeader");
      var sticky = header.offsetTop;
      function myFunction() {
          if (window.pageYOffset > sticky) {
              header.classList.add("sticky");
          } else {
            header.classList.remove("sticky");
          }
      }
    </script>

    <!-- Optional JavaScript for Bootstrap -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS-->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>

  </body>
</html>